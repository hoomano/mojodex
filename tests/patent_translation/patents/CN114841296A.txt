(19)国家知识产权局
(12)发明 专利申请
(10)申请公布号  
(43)申请公布日 
(21)申请 号  202210776495.1
(22)申请日 2022.07.04
(71)申请人 北京六方云信息技 术有限公司
地址  100085  北京市海淀区上地信息路12
号1幢2层C202室
    申请人 北京六方云科技有限公司
(72)发明人 卯路宁　
(74)专利代理 机构  北京恒程知识产权代理有限
公司  11914
专利代理师  张婷
(51)Int.Cl.
G06K 9/62(2022.01)
G06N 3/04(2006.01)
G06N 3/08(2006.01)
 
(54)发明名称
设备聚类方法、 终端设备以及存 储介质
(57)摘要
本申请公开了一种设备聚类方法、 终端设备
以及存储介质， 其设备聚类方法包括： 获取各设
备间的连接关系， 所述各设备构成图注意力网络
GAT网络的网络节点； 根据所述各设备间的连接
关系， 得到所述各设备的邻接矩阵； 对所述各设
备进行特征提取， 得到所述各设备的特征矩阵；
将所述邻接矩 阵和特征矩 阵输入至预先训练好
的图注意自编码器GATE模型中， 得到重建后的节
点特征矩阵； 将所述重建后的节 点特征矩阵输入
至聚类算法中对 各网络节点进行聚类， 得到所述
各设备的类别。 本申请提供了在设备节点增加的
情况下， 无需更新全图的节点特征并对全图进行
重新计算的设备聚类方案， 节省大量计算空间。
权利要求书3页  说明书19页  附图5页
CN 114841296 A
2022.08.02
CN 114841296 A
1.一种设备聚类方法， 其特 征在于， 所述设备聚类方法包括：
获取各设备间的连接关系， 所述各设备构成图注意力网络GAT网络的网络节点；
根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵；
对所述各设备进行 特征提取， 得到所述各设备的特 征矩阵；
将所述邻 接矩阵和特征矩阵输入至预先训练好的图注意自编码器GATE模型中， 得到重
建后的节点特 征矩阵；
将所述重建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 得到所述各
设备的类别。
2.根据权利要求1所述的设备聚类方法， 其特征在于， 所述获取各设备间的连接关系的
步骤包括：
统计GAT网络在预设时间段内的网络流 量；
获取有网络流 量流入和/或流出的设备IP；
将各设备IP的设备编号作为网络节点；
根据设备间的网络流 量交互关系给 各网络节点添加边；
基于各网络节点和边， 构建 GAT网络图结构， 得到图结构的网络设备关系。
3.根据权利要求2所述的设备聚类方法， 其特征在于， 所述对所述各设备进行特征提
取， 得到所述各设备的特 征矩阵的步骤 包括：
获取每个设备的网络进出流 量信息；
根据每个设备的网络进出流 量信息， 提取每 个设备的节点特 征；
根据每个设备的节点特 征， 生成对应的特 征矩阵。
4.根据权利要求1、 2或3所述的设备聚类方法， 其特征在于， 所述将所述邻接矩阵和特
征矩阵输入至预先训练好的图注 意自编码器GATE模型中， 得到重 建后的节 点特征矩阵的步
骤之前， 还 包括：
基于堆叠的编码器层和解码器层训练得到所述GATE模型， 具体包括：
获取预先采集的数据集， 所述数据集包括若干样本节点；
通过编码器层计算所述数据集中的各样本节点与对应的相邻节点之间的相关性， 得到
各样本节点的第一相关系数；
基于编码器层得到的第 一相关系数， 通过编码器层采用邻居的表示来生成各样本节点
的表示；
通过解码器层计算所述数据集中的各样本节点与对应的相邻节点之间的相关性， 得到
各样本节点的第二相关系数；
基于解码器层得到的第 二相关系数， 通过解码器层采用邻居的表示来重建各样本节点
的表示；
结合重建后的各样本节点的表示与 预设的目标损失函数进行模型训练， 最小化节点特
征和图结构的重建损失， 输出重建后的节点特 征矩阵， 得到训练后的GATE模型。
5.根据权利要求4所述的设备聚类方法， 其特征在于， 所述通过编码器层计算所述数据
集中的各样本节点与对应的相 邻节点之 间的相关性， 得到各样本节点的第一相关系数的步
骤包括：
采用具有节点间共享参数的自我关注机制， 通过编码器层计算得到各样本节点的第 一权　利　要　求　书 1/3 页
2
CN 114841296 A
2相关系数；
对各样本节点的第一相关系数进行归一 化处理；
所述基于编码器层得到的第 一相关系数， 通过编码器层采用邻居的表示来生成各样本
节点的表示的步骤 包括：
结合归一化处理后的第一相关系数， 通过将各样本节点的节点特征作为初始节点表
示， 采用对应的编码器层生成各样本节点的表示。
6.根据权利要求5所述的设备聚类方法， 其特征在于， 所述通过解码器层计算所述数据
集中的各样本节点与对应的相 邻节点之 间的相关性， 得到各样本节点的第二相关系数的步
骤包括：
采用具有节点间共享参数的自我关注机制， 通过解码器层计算得到各样本节点的第 二
相关系数；
对各样本节点的第二相关系数进行归一 化处理；
所述基于解码器层得到的第 二相关系数， 通过解码器层采用邻居的表示来重建各样本
节点的表示的步骤 包括：
结合归一化处理后的第 二相关系数， 通过将对应编码器层输出的样本节点的表示作为
解码器层的输入， 采用对应的解码器层重建各样本节点的表示。
7.根据权利要求6所述的设备聚类方法， 其特征在于， 所述结合重建后的各样本节点的
表示与预设的目标损失函数进行模型训练， 最小化节点特征和图结构的重建损失， 输出重
建后的节点特 征矩阵， 得到训练后的GATE模型的步骤 包括：
计算各样本节点重建后的节点表示与初始 节点表示之间的差值；
计算重建后的各样本节点与对应的相邻节点之间的表示相似；
将所述差值与 所述表示相似代入预设的目标损失函数， 计算得到节点特征和图结构的
重建损失；
将所述节点特征和图结构的重建损失回传到GATE模型， 对编码器层和解码器层的可训
练参数进行更新； 并返回执行步骤； 通过编码器层计算所述数据集中的各样本节点与对应
的相邻节点之间的相关性， 得到各样本节点的第一相关系数；
以此循环， 进行参数迭代， 最小化节点特征和图结构的重建损失， 直到所述GATE模型收
敛， 终止训练， 输出重建后的节点特 征矩阵， 得到训练后的GATE模型。
8.根据权利要求7所述的设备聚类方法， 其特征在于， 所述输出重建后的节点特征矩阵
的步骤包括：
根据编码器层得到的第一相关系数， 获取 所述编码器层的第一注意力矩阵；
根据所述第一注意力矩阵， 通过 所述编码器层输出 各样本节点的节点表示矩阵；
根据解码器层得到的第二相关系数， 获取 所述解码器层的第二注意力矩阵；
结合各样本节点的节点表示矩阵和解码器层的第 二注意力矩阵， 通过解码器层输出重
建后的节点特 征矩阵。
9.一种终端设备， 其特征在于， 所述终端设备包括存储器、 处理器及存储在所述存储器
上并可在所述处理器上运行的设备聚类程序， 所述设备聚类程序被所述处理器执行时实现
如权利要求1 ‑8中任一项所述的设备聚类方法的步骤。
10.一种计算机可读存储介质， 其特征在于， 所述计算机可读存储介质上存储有设备聚权　利　要　求　书 2/3 页
3
CN 114841296 A
3类程序， 所述设备聚类程序被处理器执行时实现如权利要求1 ‑8中任一项所述的设备聚类
方法的步骤。权　利　要　求　书 3/3 页
4
CN 114841296 A
4设备聚类方 法、 终端设 备以及存 储介质
技术领域
[0001]本申请涉及网络安全检测领域， 尤其涉及一种设备聚类方法、 终端设备以及存储
介质。
背景技术
[0002]随着计算机技术的发展， 全球信息化已经成为人类发展的大趋势。 但由于计算机
网络具有连接形式多样性、 终端分布 不均匀和网络的开放性、 互联性等特征， 致使网络容易
受到黑客、 恶意软件等的攻击。 网络 设备作为互联网中重要的元素， 通过对设备之间的通信
关系进行分析， 普遍认为有相似通信行为的设备间具有较高的相似度。 通过对有相似通信
行为的设备进行聚类， 可将设备按照相似性分类， 这对于研究设备间关系及设备 的异常行
为至关重要。 目前已有的设备聚类技术大多基于机器学习， 通过提取各个设备 的行为作为
特征， 然后利用决策树分类、 无监督聚类等方法， 对设备进行分组画像。 然而在实现本申请
过程中， 发明人发现采用现有设备聚类技术进行设备聚类时， 由于其学习的参数与图结构
有关， 因此每当有设备节点增加时， 就需要在每一次计算时更新全图的节点特征， 致使其需
要耗费大量的计算资源， 因而难以完成动态变化的设备聚类任务。
[0003]因此， 有必要提出一种无需更新全图的节点特征并对全图进行重新计算的设备聚
类解决方案 。
发明内容
[0004]本申请的主要 目的在于提供一种设备聚类方法、 终端设备以及存储介质， 旨在提
供一种在设备节点增加的情况下， 无需更新全图的节点特征并对全图进 行重新计算的设备
聚类方案， 节省大量计算空间。
[0005]为实现上述目的， 本申请提供一种设备聚类方法， 所述设备聚类方法包括：
获取各设备间的连接关系， 所述各设备构成图注意力网络GAT网络的网络节点；
根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵；
对所述各设备进行 特征提取， 得到所述各设备的特 征矩阵；
将所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编码器GATE模型中， 得
到重建后的节点特 征矩阵；
将所述重建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 得到所
述各设备的类别。
[0006]可选地， 所述获取 各设备间的连接关系的步骤 包括：
统计GAT网络在预设时间段内的网络流 量；
获取有网络流 量流入和/或流出的设备IP；
将各设备IP的设备编号作为网络节点；
根据设备间的网络流 量交互关系给 各网络节点添加边；
基于各网络节点和边， 构建 GAT网络图结构， 得到图结构的网络设备关系。说　明　书 1/19 页
5
CN 114841296 A
5[0007]可选地， 所述对所述各设备进行特征提取， 得到所述各设备的特征矩阵的步骤包
括：
获取每个设备的网络进出流 量信息；
根据每个设备的网络进出流 量信息， 提取每 个设备的节点特 征；
根据每个设备的节点特 征， 生成对应的特 征矩阵。
[0008]可选地， 所述将所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编码器
GATE模型中， 得到 重建后的节点特 征矩阵的步骤之前， 还 包括：
基于堆叠的编码器层和解码器层训练得到所述GATE模型， 具体包括：
获取预先采集的数据集， 所述数据集包括若干样本节点；
通过编码器层计算所述数据集中的各样本节点与对应的相邻节点之间的相关性，
得到各样本节点的第一相关系数；
基于编码器层得到的第一相关系数， 通过编码器层采用邻居的表示来生成各样本
节点的表示；
通过解码器层计算所述数据集中的各样本节点与对应的相邻节点之间的相关性，
得到各样本节点的第二相关系数；
基于解码器层得到的第二相关系数， 通过解码器层采用邻居的表示来重建各样本
节点的表示；
结合重建后的各样本节点的表示与预设的目标损失函数进行模型训练， 最小化节
点特征和图结构的重建损失， 输出重建后的节点特 征矩阵， 得到训练后的GATE模型。
[0009]可选地， 所述通过编码器层计算所述数据集中的各样本节点与对应的相邻节点之
间的相关性， 得到各样本节点的第一相关系数的步骤 包括：
采用具有节点间共享参数的自我关注机制， 通过编码器层计算得到各样本节点的
第一相关系数；
对各样本节点的第一相关系数进行归一 化处理；
所述基于编码器层得到的第一相关系数， 通过编码器层采用邻居的表示来生成各
样本节点的表示的步骤 包括：
结合归一化处理后的第一相关系数， 通过将各样本节点的节点特征作为初始节点
表示， 采用对应的编码器层生成各样本节点的表示。
[0010]可选地， 所述通过解码器层计算所述数据集中的各样本节点与对应的相邻节点之
间的相关性， 得到各样本节点的第二相关系数的步骤 包括：
采用具有节点间共享参数的自我关注机制， 通过解码器层计算得到各样本节点的
第二相关系数；
对各样本节点的第二相关系数进行归一 化处理；
所述基于解码器层得到的第二相关系数， 通过解码器层采用邻居的表示来重建各
样本节点的表示的步骤 包括：
结合归一化处理后的第二相关系数， 通过将对应编码器层输出的样本节点的表示
作为解码器层的输入， 采用对应的解码器层重建各样本节点的表示。
[0011]可选地， 所述结合重建后的各样本节点的表示与预设的目标损失函数进行模型训
练， 最小化节 点特征和图结构的重 建损失， 输出重 建后的节 点特征矩阵， 得到训练后的GATE说　明　书 2/19 页
6
CN 114841296 A
6模型的步骤 包括：
计算各样本节点重建后的节点表示与初始 节点表示之间的差值；
计算重建后的各样本节点与对应的相邻节点之间的表示相似；
将所述差值与所述表示相似代入预设的目标损失函数， 计算得到节点特征和图结
构的重建损失；
将所述节点特征和图结构的重建损失回传到GATE模型， 对编码器层和解码器层的
可训练参数进行更新； 并返回执行步骤； 通过编码器层计算所述数据集中的各样本节点与
对应的相邻节点之间的相关性， 得到各样本节点的第一相关系数；
以此循环， 进行参数迭代， 最小化节点特征和图结构的重建损失， 直到所述GATE模
型收敛， 终止训练， 输出重建后的节点特 征矩阵， 得到训练后的GATE模型。
[0012]可选地， 所述输出重建后的节点特 征矩阵的步骤 包括：
根据编码器层得到的第一相关系数， 获取 所述编码器层的第一注意力矩阵；
根据所述第一注意力矩阵， 通过 所述编码器层输出 各样本节点的节点表示矩阵；
根据解码器层得到的第二相关系数， 获取 所述解码器层的第二注意力矩阵；
结合各样本节点的节点表示矩阵和解码器层的第二注意力矩阵， 通过解码器层输
出重建后的节点特 征矩阵。
[0013]本申请实施例还提出一种设备聚类装置， 所述设备聚类装置包括：
关系获取模块， 用于获取各设备 间的连接关系， 所述各设备构成图注意力网络GAT
网络的网络节点；
邻接矩阵模块， 用于根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵；
特征矩阵模块， 用于对所述各设备进行 特征提取， 得到所述各设备的特 征矩阵；
图注意力模块， 用于将 所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编
码器GATE模型中， 得到 重建后的节点特 征矩阵；
设备聚类模块， 用于将 所述重建后的节点特征矩阵输入至聚类算法中对各网络节
点进行聚类， 得到所述各设备的类别。
[0014]本申请实施例还提出一种 终端设备， 所述终端设备包括存储器、 处理器及存储在
所述存储器上并可在所述处理器上运行的设备聚类程序， 所述设备聚类程序被所述处理器
执行时实现如上 所述的设备聚类方法的步骤。
[0015]本申请实施例还提出一种计算机可读存储介质， 所述计算机可读存储介质上存储
有设备聚类程序， 所述设备聚类程序被处理器执行时实现如上所述的设备聚类方法的步
骤。
[0016]本申请实施例提出的设备聚类方法、 终端设备以及存储介质， 通过获取各设备间
的连接关系， 所述各设备构成图注 意力网络 GAT网络的网络节点； 根据所述各设备间的连接
关系， 得到所述各设备的邻接矩阵； 对 所述各设备进 行特征提取， 得到所述各设备的特征矩
阵； 将所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编码器GATE模型中， 得到重
建后的节点特征矩阵； 将所述重建后的节点特征矩阵输入至聚类算法中对各节点进行聚
类， 得到所述各设备的类别。 基于本申请方案， 从相似通信行为的设备间具备较高相似度的
规则出发， 构建了一个能将设备按照相似性进行分类的设备聚类模型， 基于本申请构建的
设备聚类模型， 在设备增加的情况下， 无需更新全图的节点特征并对全图进行重新计算， 最说　明　书 3/19 页
7
CN 114841296 A
7后经过本申请方案实现设备聚类的方法节省了大量计算空间。
附图说明
[0017]图1为本申请设备聚类装置所属终端设备的功能模块 示意图；
图2为本申请设备聚类方法第一 示例性实施例的流 程示意图；
图3为本申请设备聚类方法第二 示例性实施例的流 程示意图；
图4为本申请设备聚类方法第三 示例性实施例的流 程示意图；
图5为本申请设备聚类方法实施例涉及的GATE模型的一种训练流 程示意图；
图6为本申请实施例中结合重建后的各样本节点的表示与预设的目标损失函数进
行模型训练， 最小化节点特征和图结构的重 建损失， 输出重建后的节点特征矩阵， 得到训练
后的GATE模型的具体流 程示意图；
图7为本申请实施例中输出重建后的节点特 征矩阵的具体流 程示意图；
图8为本申请设备聚类方法第五示例性实施例的整体流 程示意图；
图9为本申请实施例中将所述重建后的节点特征矩阵输入至聚类算法中对各网络
节点进行聚类， 得到所述各设备的类别的具体流 程示意图。
[0018]本申请目的 的实现、 功能特点及优点将结合实施例， 参照附图做进一 步说明。
具体实施方式
[0019]应当理解， 此处所描述的具体实施例仅 仅用以解释本申请， 并不用于限定 本申请。
[0020]本申请实施例的主要解决方案是： 获取各设备间的连接关系， 所述各设备构成图
注意力网络 （GAT网络） 的网络节点； 根据所述各设备间的连接 关系， 得到所述各设备的邻接
矩阵； 对所述各设备进 行特征提取， 得到所述各设备的特征矩阵。 通过在GATE模型中配置堆
叠的编码器层和解码器层， 获取预先采集的数据集， 所述数据集包括若干样 本节点； 通过编
码器层计算所述数据集中的各样本节点与对应的相邻节点之间的相关性， 得到各样本节点
的第一相关系 数； 基于编码器层得到的第一相关系 数， 通过编码器层采用邻居的表示来生
成各样本节点的表示； 通过解码 器层计算所述数据集中的各样本节点与对应的相 邻节点之
间的相关性， 得到各样本节点的第二相关系数； 基于解码 器层得到的第二相关系数， 通过解
码器层采用邻居的表示来重 建各样本节点的表示； 结合重 建后的各样本节点的表示与预设
的目标损失函数进行模型训练， 最小化节点特征和图结构的重建损失， 输出重建后的节点
特征矩阵， 得到训练后的GATE模型。 通过训练后的GATE模型对所述各网络节点的邻接矩阵
和特征矩阵进行处理后， 得到重建后的节点特征矩阵， 将所述重建后的节点特征矩阵输入
至聚类算法中对各网络节 点进行聚类， 得到所述各设备的类别。 基于本申请方案， 从相似通
信行为的设备间具备较高相似度的规则出发， 构建了一个基于 GATE模型并将设备按照相似
性进行分类的设备聚类模型， 基于本申请构建的设备聚类模型， 在设备节点增加的情况下，
无需更新全图的节点特征并对全图进 行重新计算， 最后经过本申请方案实现设备聚类的方
法节省了大量计算空间。
[0021]本申请实施例涉及的技 术术语：
注意力机制， A ttention；
自注意力机制， Self  Attention；说　明　书 4/19 页
8
CN 114841296 A
8图卷积神经网络， GCN， Graph  Convolutional Network；
图注意力网络， GAT， Graph  Attention Network；
图注意自编码器， GATE， Graph  Attention Auto‑encoder。
[0022]其中， 注意力机制 （Attention） 是聚焦于局部信息的机制。 而随着任务的变化， 注
意力区域往往会发生变化， 而注意力机制便是要找到任务所需的最有用的信息。 近年来， 注
意力机制在图像、 自然语言处理等领域都取得了重要的突破， 被证明有益于提高模型 的性
能。 注意力机制本质上就是定位到感兴趣的信息， 抑制无用信息， 结果通常都是以概率图或
者概率特征向量的形式展示。 深度学习中的注意力机制从本质上来讲和人类的选择性视觉
注意力机制类似， 核心目标也是从众多信息中选择出对当前任务 目标更为关键的信息。 所
以， 注意力机制的目的是根据我们的目标， 去关注部分细节， 而不是基于全局进行分析， 所
以其核心就是如何基于目标确定我们要关注我部分， 以及在找到这部 分细节之后做进一步
地分析。 基于注意力机制构建的注意力模 型 （AM， Attention  Model） 通过 允许模型动态地关
注有助于执 行手头任务的输入的某些部分， 将这种相关性 概念结合 起来。
[0023]自注意力机制 （也叫自我关注机制， Self  Attention） 是注意力机制中的一种， 也
是转换器 （Transformer） 中的重要组成部分。 自注 意力机制要解决 的问题是： 当神经网络的
输入是多个大小不一样的向量， 并且可能因为不同向量之间有一定的关系， 而在训练时却
无法充分发挥这些关系， 导 致模型训练结果较差 。
[0024]图卷积神经网络 （GCN， Graph  Convolutional  Network） 是一种特征提取器， 其对
象为图数据， 即图卷积神经网络就是一种处理图数据的深度学习方法。 GCN精 妙地设计了一
种从图数据中提取特征的方法， 从而可以使用这些特征去对图数据进行节点分类 （Node  
Classification） 、 图分类 （Graph  Classification） 、 边预测 （Link  Prediction） ， 还可以顺
便得到图的嵌入表示 （Graph  Embedding）,用途广泛。 然而， 由于GCN模型在训练期间需要整
个图里面所有的节点参与， 即需要将整个具体的图作为输出， 所以改变图或者节点时， GCN
模型就需要从头开始训练。
[0025]图注意力网络 （也叫图注意力机制， GAT， Gr aph Attention  Network） 是一种基于
图结构数据的新型神经网络架构， 利用隐藏的自注意力层来解决之前基于图卷积或者近似
方法的不足。 GAT引入了注意力机制 （Attention） 来实现更好的邻居聚合， 通过学习邻居的
权重， GAT可以实现对邻居的加 权聚合。 GAT通过堆叠层， 使节点能够参与邻居的特征， 可以
（隐式地） 为邻域中的不同节点指定不同的权值， 而不需要任何代价高昂的矩阵操作 （如反
转） ， 也不需要预先知道图的结构。 通过这种 方法， 该模型克服了基于频谱的神经网络的几
个关键挑战， 并使得模型适用于归纳和推理问题。
[0026]对比图卷积神经网络 （GCN， Graph  Convolutional  Network） 和图注意力网络
（GAT， Graph  Attention  Network） ， GAT由自注意力层构建而成， 自注意力层能够解决先前
使用神经网络对图结构数据建模方法中存在的问题， 包括： 计算更高效， 自注 意力层的操作
可以在所有边之间并行进行， 同时所有节点的输出特征 的计算也可以并行， 不需要特征分
解以及类似代 价较高的矩阵操作； GAT模 型与GCN模 型相反， GAT模 型允许 （隐式地） 将不同的
重要性分配给同一邻居的节点， 从而实现了模型容量的提升， 此外， 分析学习到的注意力权
重可能会带来可解释性方面的好处； 图注意力机制以共享的方式应用于图中的所有边， 因
此它不依赖于对全局图结构或者其所有节点 （特性） 的预先访问， 即， 图不需要 是无向的， 且说　明　书 5/19 页
9
CN 114841296 A
9它使所提技术可以直接用于归纳学习， 包括在训练过程中完全看不见的图上评估模型的任
务。
[0027]图注意自编码 器 （GATE， Graph  Attention Auto‑encoder） 是一个基于图结构数据
进行无监督学习的神经网络架构。 此神经网络架构能够通过结合自注意力机制 （Self  
Attention） 的堆叠的编码 器层/解码 器层重构图结构化输入， 包括节点属性和图结构 。 在编
码器层中， 通过考虑节点属 性作为初始节点表示， 每一层通过关注其邻居的表产生新的节
点表示。 在解码 器层中， 通过反转编码过程来重构节点属性。 此外通过正则化节点表示来重
构图结构。 此外， 神经网络架构不需要预 先知道图结构， 因此 可以应用于归纳学习。
[0028]归一化， Normalization； Sigmoid函数； Softmax函数。
[0029]其中， 归一化 （Normalization） 也称数据规范化。 在机器学习 领域中， 不同评价指
标 （即特征向量中的不同特征就是所述的不同评价指标） 往往 具有不同的量纲和量纲单位，
这样的情况会影响到数据分析 的结果。 为了消除指标之间的量纲影响， 需要进行数据归一
化处理， 将数据按照相应比例进 行缩放， 使之落入一个特定的区域 （映射于指 定区域） ， 以解
决数据指标之 间的可比性。 原始数据经过数据归一化处理后， 各指标处于同一数量级， 适合
进行综合对比评价。 归一化的目的就是使 得预处理的数据被限定在一定的范围内， 比如[ 0,
1]或者[‑1,1]， 从而消除奇异样本数据导致的不良影响。 其中奇异样本数据是指相对于其
他输入样本特别大或者特别小的样本矢量 （即特征向量） 。 通常神经网络需要归一化处理，
一般变量的取值在 ‑1到1之间， 这样做是为了弱化某些变量的值较大而对 模型产生影响。
[0030]Sigmoid函数， 也称S型生长曲线， 是神经元的非线性作用函数。 Sigmoid函数在 （0,
0.5） 处中心对称， 在(0,  0.5)附近有比较大的斜率， 而当数据趋向于正无穷和负无穷的时
候， 映射出来的值就会无限趋向于1和0。 在深度学习中， 由于其单增以及反函数单增等性
质， Sigmoid函数常被用作神经网络的激活函数， 将 变量映射到[ 0,1]之间。 激活函数给神经
元引入了非线性因素， 当加入多层神经元网络时， 可以让神经网络拟合任何线性函数及非
线性函数， 从而 使得神经网络可以适用于更多的非线性问题， 而不仅 仅是线性问题。
[0031]Softmax函数， 又称归一化指数函数， 它是二分类函数 ‑sigmoid函数在多分类上的
推广， 目的是将实数范围内的分类结果转化为0 ‑1之间的概率， 通过利用指数的特性， 将实
数映射到0 ‑正无穷 （非负） ， 并利用归一化方法， 将实数映射的结果转化为0 ‑1之间的概率。
通常单个输出节点的二分类问题一般在输出节点上使用Sigmoid函数， 拥有两个及其以上
的输出节点的二分类或者多分类问题一般在输出节点上使用Softmax函数。
[0032]K‑means算法、 肘部法则：
其中， K‑means算法又叫K均值算法。 K ‑means算法中的K表示的是聚类为K个簇，
means表示取每一个聚类中数据值的均值作为该簇的中心， 或称为质心， 即用每一个类的质
心对该簇进行描述。 K ‑means算法的思想大致为： 先从样本集中随机选取K个样本作为簇中
心， 并计算所有样本与这K个簇中心的距离， 对于每一个样本， 将其划分到与其距离最近的
簇中心所在的簇中， 对于新的簇计算各个簇的新的簇中心， 直到 簇中心没有移动。
[0033]肘部法则， 一种K ‑means聚类的K值选择规则。 肘部法则的核心思想是分类数K越
大， 样本划分会更加精细， 每个类的聚合程度会逐渐提高， 那么误差平方和SEE自然会逐渐
变小。 肘部法则的计算原理是成本函数， 成本函数是类别畸变程度之和， 每个类的畸变程度
等于每个变量点到其类别中心的位置距离平 方和 （类内部的成员彼此越紧凑则类的畸变程说　明　书 6/19 页
10
CN 114841296 A
10度越小， 越分散越 大） 。 在选择类别数量上， 肘部法则会把不同值的成本函数值画出来。 随着
值的增大， 每个类包含的样 本数会减少， 于是样本离其重心会 更近平均畸变程度会减小。 随
着值继续增大， 平均畸变程度的改善效果会不断减低。 值增大过程中， 畸变程度的改善效果
下降幅度最大的位置对应的值就是肘部。 而这个值就可以考虑为聚类性能较好的点。 因此，
肘部法则对于K ‑means算法的K值确定起到指导作用。
[0034]本申请实施例 方案， 从相似通信行为的设备间具备较高相似度的规则出发， 构建
了一个基于 GATE模型并将设备按照相似性进 行分类的设备聚类模型， 基于本申请构建的设
备聚类模 型， 在设备节点增加的情况下， 无需更新全图的节 点特征并对全图进 行重新计算，
最后经过本申请方案实现设备聚类的方法节省了大量计算空间。
[0035]具体地， 参照图1， 图1为本申请设备聚类装置所属终端设备的功能模块示意图。 该
设备聚类装置可以为独立于终端设备的、 能够进 行设备聚类、 网络模 型训练的装置， 其可以
通过硬件或软件的形式承载于终端设备上。 该终端设备可以为手机、 平板电脑等具有数据
处理功能的智能移动终端， 还可以为具有数据处 理功能的固定终端设备或服 务器等。
[0036]在本实施例中， 该设备聚类装置所属终端 设备至少包括输出模块110、 处理器120、
存储器130以及通信模块140 。
[0037]存储器130中存储有操作系统 以及设备聚类程序， 设备聚类装置可以将获取的各
设备间的连接关系， 通过各设备间的连接关系得到的各设备 的邻接矩阵， 通过对设备进行
特征提取得到的特征矩阵， 以及获取 的预先采集的包括节点和相邻节点的数据集， 通过编
码器层对节点和相邻节点的相关性进 行计算得到节点的第一相关系数， 通过编 码器层采用
邻居的表示来生成的节点的表示， 通过解码 器层对节点和相 邻节点的相关性进行计算得到
节点的第二相关系 数， 通过解码器层采用邻居的表示来重建节点的表示， 通过结合所述重
建后的节点表示与预设的目标损失函数训练所得的最小化节点特征和图结构的重建损失，
通过训练好的GATE模 型输出重 建后的节 点特征矩阵， 通过将所述重 建后的节 点特征矩阵输
入至聚类算法中对各节点进行聚类得到的各设备 的类别等信息存储于该存储器130中； 输
出模块110可为显示屏等。
[0038]通信模块140可以包括WIFI模块、 移动通信模块以及蓝牙模块等， 通过通信模块
140与外部设备或服 务器进行通信。
[0039]其中， 存储器130中的设备聚类程序被处 理器执行时实现以下步骤：
获取各设备间的连接关系， 所述各设备构成图注意力网络GAT网络的网络节点；
根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵；
对所述各设备进行 特征提取， 得到所述各设备的特 征矩阵；
将所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编码器GATE模型中， 得
到重建后的节点特 征矩阵；
将所述重建后的节点特征矩阵输入至聚类算法中对各节点进行聚类， 得到所述各
设备的类别。
[0040]基于上述终端设备架构但不限于上述架构， 提出本申请方法实施例。
[0041]参照图2， 图2为本申请设备聚类方法第一示例性实施例的流程示意图。 所述设备
聚类方法包括：
步骤S11， 获取各设备间的连接关系， 所述各设备构成图注意力网络GAT网络的网说　明　书 7/19 页
11
CN 114841296 A
11络节点。
[0042]具体地， 获取各设备间的连接关系， 其中， 各设备间的连接关系是指设备间因存在
相互通信行为而相互连接的关系， 包括但不限于设备间的网络流量交互关系。 其中， 所述各
设备构成GAT网络的网络节 点， 在本实施例中表 示为可将所有设备看做是图G= （E,V） 中的节
点， 节点之间的连接关系可看做是边， 由节点和边共同构成了GAT网络。 更进一步地， 采用
GAT网络的原因在于： GAT网络中重要的学习参数是W和α （.） ， 这两个参数仅与节点特征相
关， 与图的结构毫无关系， 所以改变图的结构时， 对于 GAT网络的影响不大， 因此更适合于归
纳式 （inductive） 任务。 而其他神经网络算法， 如GCN， 是一种全图的计算方式， 由于其学习
的参数很大程度上与图结构相关， 每一次计算都要更新全图的节点特征， 使得GCN在
inductive任务上遇 到困难。
[0043]步骤S21， 根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵。
[0044]具体地， 根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵， 其中， 为各
设备添加设备编号作为节点， 为各设备的连接 关系添加边， 由节点和边共同构成GAT网络的
图结构， 通过所得图结构计算得到各设备节点的邻接矩阵。 更进一步地， 邻接矩阵是表示节
点之间相邻关系的矩阵， 对于一个具有N （N∈R） 个节点的图来说， 邻接矩阵A为一个大小为
N*N的对称矩阵， 主对角线为0， 若两个节点 i， j之间有连接， 则Aij=Aji=1， 否则为0 。
[0045]步骤S31， 对所述各设备进行 特征提取， 得到所述各设备的特 征矩阵。
[0046]具体地， 提取各设备的特征， 其中， 设备的特征是与设备间的相互通信行为相关的
特征， 包括但不限于流入/流出的流量大小、 端口信息、 协 议类型等。 将提取的各设备特征作
为节点特 征， 形成对应排列的特 征矩阵。
[0047]步骤S41， 将所述邻接矩阵和特征矩阵输入至预先训练好 的图注意自编码器GATE
模型中， 得到 重建后的节点特 征矩阵。
[0048]具体地， 将基于图结构得到的邻接矩阵和基于节点特征得到的特征矩阵输入到预
先训练好的GATE模型中， 其中， GATE模型采用深度学习自编码器结合引入了注意力机制
（Attention） 的GAT网络， 通过预先采集的数据集完成训练。 将邻接矩阵和特征矩阵输入至
训练好的GATE模型中， 根据 节点与相邻节点的相关性利用其邻居的表示来 实现节点特征和
图结构的重建， 输出重建后的节点特 征矩阵。
[0049]步骤S51， 将所述重建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚
类， 得到所述各设备的类别。
[0050]具体地， 由于经GATE模型输出的网络节点不具备类别标签， 因此需要将重建后的
节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 根据聚类结果得到各设备的类
别， 其中， 可采用的聚类算法包括但不限于K ‑means算法。
[0051]本实施例方法的执行主体可以是一种设备聚类装置， 也可以是一种设备聚类终端
设备或服务器， 本实施例以设备聚类装置进行举例， 该设备聚类装置可以集成在具有数据
处理功能的智能手机、 平板电脑等终端设备 上。
[0052]本实施例通过上述方案， 具体通过获取各设备间的连接关系， 所述各设备构成图
注意力网络 GAT网络的网络节 点； 根据所述各设备间的连接 关系， 得到所述各设备的邻接矩
阵； 对所述各设备进 行特征提取， 得到所述各设备的特征矩阵； 将所述邻接矩阵和特征矩阵
输入至预先训练好的图注意自编码器GATE模型中， 得到重建后的节点特征矩阵； 将所述重说　明　书 8/19 页
12
CN 114841296 A
12建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 得到所述各设备 的类别。
基于本申请方案， 从相似通信行为的设备间具备较高相似度的规则出发， 构建了一个能将
设备按照相似性进行分类的设备聚类模型， 基于本申请构建的设备聚类模型， 在设备增加
的情况下， 无需更新全图的节点特征并对全图进行重新计算， 最后经过本申请方案实现设
备聚类的方法节省了大量计算空间。
[0053]参照图3， 图3为本申请设备聚类方法第二示例性实施例的流程示意图。 基于上述
图2所示的实施例， 在本实施例中， 上述步骤S11， 获取各设备间的连接关系， 所述各设备构
成图注意力网络GAT网络的网络节点， 可以包括：
步骤S111， 统计GAT网络在预设时间段内的网络流 量。
[0054]具体地， 统计GAT网络中各资产在预设时间段内的网络流量， 其中， 资产包括但不
限于系统程序、 应用程序等软件产品， 资产需承托在智能手机、 电脑等设备上运行， 产生网
络流量。 其中， 网络流量的具体形式包括： 流入该资产的网络流量、 该资产向其他设备发送
的网络流 量。
[0055]步骤S112， 获取有网络流 量流入和/或流出的设备IP。
[0056]具体地， 获取有网络流量流入和/或流出的设备IP， 其中， 每个设备都有其固定的
IP， 通过统计资产的网络流量流入和 /或流出信息， 获取承载该资产的设备的IP， 具体形式
包括： 获取该资产的设备IP作为第一列表、 获取有网络流量流入该资产的设备IP作为第二
列表、 获取接收了该资产流出的网络流 量的其他设备IP作为第三列表， 如表一所示：
资产IP    流入IP    流出IP    
A  [C,E...] [B,D...] 
表一： 网络流 量流入和/或流出设备IP表
其中A、 B、 C、 D、 E均为设备， 表示 为设备A和B、 C、 D、 E都有网络流 量交互关系。
[0057]步骤S113， 将各设备IP的设备编号作为网络节点。
[0058]具体地， 将获取到的设备IP添加设备编码， 并使用设备编码作为GAT网络的网络节
点。
[0059]步骤S114， 根据设备间的网络流 量交互关系给 各网络节点添加边。
[0060]具体地， 基于获取的网络流量流入和/或流出设备IP表， 得到设备间的网络流量交
互关系。 根据设备间的网络流 量交互关系给 各网络节点添加边。
[0061]步骤S115， 基于各网络节点和边， 构建GAT网络图结构， 得到图结构的网络设备关
系。
[0062]具体地， 本实施例基于GAT网络中各节点间的网络流量流入和/或流出信息， 获得
节点间的网络流量交互关系， 并根据网络流量交互关系给各节点添加边， 构建GAT网络图结
构， 得到图结构的设备关系。
[0063]进一步地， 基于上述图2所示 的实施例， 在本实施例中， 上述步骤S21， 根据所述各
设备间的连接 关系， 得到所述各设备的邻接矩阵具体为： 根据所述图结构的网络 设备关系，
计算得到各网络节点的邻接矩阵。 更为具体地， 邻接矩阵是表示图结构的网络设备关系中
各网络节点之间相邻关系的矩阵。
[0064]本实施例通过上述方案， 具体通过获取各设备间的连接关系， 所述各设备构成图
注意力网络 GAT网络的网络节 点； 根据所述各设备间的连接 关系， 得到所述各设备的邻接矩说　明　书 9/19 页
13
CN 114841296 A
13阵； 对所述各设备进 行特征提取， 得到所述各设备的特征矩阵； 将所述邻接矩阵和特征矩阵
输入至预先训练好的图注意自编码器GATE模型中， 得到重建后的节点特征矩阵； 将所述重
建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 得到所述各设备 的类别。
基于本申请方案， 从相似通信行为的设备间具备较高相似度的规则出发， 构建了一个能将
设备按照相似性进行分类的设备聚类模型， 基于本申请构建的设备聚类模型， 在设备增加
的情况下， 无需更新全图的节点特征并对全图进行重新计算， 最后经过本申请方案实现设
备聚类的方法节省了大量计算空间。
[0065]进一步地， 参照图4， 图4为本申请设备聚类方法第三示例性实施例的流程示意图。
基于上述图3所示的实施例， 在本实施例中， 上述步骤S31， 对所述各设备进行特征提取， 得
到所述各设备的特 征矩阵可以包括：
步骤S311， 获取每 个设备的网络进出流 量信息。
[0066]具体地， 获取每个设备的网络进出流量信息， 其中， 网络进出流量信息包括但不限
于每个设备使用的网络协议， 以及每个设备下 的每种网络协议的进出流量大小。 在本实施
例中， 统计了 常见的网络协 议及其进出流量大小， 其中， 统计的网络协 议包括但 不限于ARP、
DNS、 FTP、 IMAP、 H HTPS、 POP3、 RD P、 SIP、 SMB、 SMTP、 SNMP、 S SH、 ICMP。
[0067]步骤S312， 根据每 个设备的网络进出流 量信息， 提取每 个设备的节点特 征。
[0068]具体地， 根据每个设备的网络协议的进出流量信息， 以每种协议的进出流量大小
作为该设备的节点特 征进行提取。
[0069]步骤S313， 根据每 个设备的节点特 征， 生成对应的特 征矩阵。
[0070]具体地， 根据提取到的设备的节点特征， 生成对应排列的特征矩阵， 其中， 可以以
节点数作为特征矩阵的行数， 以网络协议数作为特征矩阵的列数， 根据每个设备 的节点特
征生成特 征矩阵的元 素。
[0071]本实施例通过上述方案， 具体通过获取各设备间的连接关系， 所述各设备构成图
注意力网络 GAT网络的网络节 点； 根据所述各设备间的连接 关系， 得到所述各设备的邻接矩
阵； 对所述各设备进 行特征提取， 得到所述各设备的特征矩阵； 将所述邻接矩阵和特征矩阵
输入至预先训练好的图注意自编码器GATE模型中， 得到重建后的节点特征矩阵； 将所述重
建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 得到所述各设备 的类别。
基于本申请方案， 从相似通信行为的设备间具备较高相似度的规则出发， 构建了一个能将
设备按照相似性进行分类的设备聚类模型， 基于本申请构建的设备聚类模型， 在设备增加
的情况下， 无需更新全图的节点特征并对全图进行重新计算， 最后经过本申请方案实现设
备聚类的方法节省了大量计算空间。
[0072]参照图5， 图5为本申请设备聚类方法实施例涉及的GATE模型的一种训练流程示意
图。 在本实施例中， 在上述步骤S41， 将所述邻接矩阵和特征矩阵输入至预先训练好的图注
意自编码器GATE模型中， 得到 重建后的节点特 征矩阵之前， 还可以包括：
步骤S40， 基于堆叠的编码器层和解码器层训练得到所述GATE模型。
[0073]本实施例以步骤S40在步骤S41之前实施。 具体地， GATE模型由GAT网络和自编码器
结合而成， 其框架使用了自编码器的模型框架， 包括堆叠的编码器层和解码器层， 其中， 每
一层的系数使用了注意力系数的计算方法。 GATE模型采用预先采集的数据集， 通过堆叠的
编码器层和解码器层训练得到 。说　明　书 10/19 页
14
CN 114841296 A
14[0074]相比上述图4所示的实施例， 本实施例还 包括训练GATE模型的方案 。
[0075]具体地， 步骤S40， 基于堆叠的编码器层和解码器层训练得到所述GATE模型， 具体
包括：
步骤S401， 获取 预先采集的数据集， 所述数据集包括若干样本节点。
[0076]更为具体地， 本实施例预先收集了若干数量的样本节点， 组成样本数据集； 将样本
数据集划分为训练集和测试集， 所述训练集和测试集均包括若干样本节点， 其中， 训练集用
于对GATE模型进行训练， 测试集用于对训练过的GATE模型进行测试， 以验证GATE模型的节
点特征矩阵的生成效果。
[0077]步骤S402， 通过编码器层 计算所述数据集中的各样本节点与 对应的相邻节点之间
的相关性， 得到各样本节点的第一相关系数。
[0078]具体地， 在训练GATE模型时， 将预先采集的训练集输入GATE模型中进行训练， 将各
样本节点与对应的相 邻节点输入至编 码器层中， 编 码器层通过计算各样本节点与对应的相
邻节点之间的相关性， 得到各样本节点的第一相关系数。
[0079]进一步地， 步骤S402， 通过编码器层计算所述数据集中的各样本节点与对应的相
邻节点之间的相关性， 得到各样本节点的第一相关系数， 可以包括：
采用具有节点间共享参数的自我关注机制， 通过编码器层计算得到各样本节点的
第一相关系数；
对各样本节点的第一相关系数进行归一 化处理。
[0080]更为具体地， 为了确定数据集中各样本节点与其邻居 （即相邻节点） 之间的相关
性， 采用具有节点间共享参数 的自我关注机制 （又称自注意力机制） ， 通过编码器层计算该
样本节点与其相邻节点之间的相关系数， 并称作第一相关系数。 在第k个编码器层中， 相邻
节点j和节点 i的相关性计算如下公式1所示：
（1）
其中， eij(k)是第k个编码器层中相邻节点j与节点i的第一相关系数/注意系数， W、
vs、 vr是可训练参数， σ 表示激活函数， Sigmoid表示Sigmoid函数， hi(k‑1)是第k‑1个编码器层
生成的节点 i的表示， hj(k‑1)是第k‑1个编码器层生成的节点j的表示。
[0081]为了使节点i 的邻域的相关系数具有可比性， 采用Softmax函数进行归一化处理，
如下公式2所示：
（2）
其中， αij(k)是第k个编码器层中相邻节点j与节点i的归 一化第一相关系数/注意系
数， eij(k)是第k个编码器层中相邻节点j与节点i的第一相关系数/注意系数， Ni是包括节点i
的节点i的邻域， l是节点 i的邻域内任一相邻节点。
[0082]通过上述公式1和2可知， 在本实施例中， 采用了具有节点间共享参数的自我关注
机制计算得到相 邻节点j和节点i的第一相关系数， 并为了使节点i的相关系数具有 可比性，
对第一相关系数进行了归一 化处理。说　明　书 11/19 页
15
CN 114841296 A
15[0083]步骤S403， 基于编码器层得到的第一相关系数， 通过编码器层采用 邻居的表示来
生成各样本节点的表示， 可以包括：
结合归一化处理后的第一相关系数， 通过将各样本节点的节点特征作为初始节点
表示， 采用对应的编码器层生成各样本节点的表示。
[0084]具体地， 通过将该样本节点的节点特征作为初始节点表示 （即hi（0）=xi，∀i∈{1，
2， .， N}） ， 编码器在第k层生成节点 i的表示， 具体 计算过程如下公式3所示：
（3）
其中， hi(k)是第k个编码器层生成的节点i的表示， hj(k‑1)是第k‑1个编码器层生成
的节点j的表示， αij(k)是第k个编码器层中相邻节点j与节点i的归一化第 一相关系数/注意
系数， W是 可训练参数， σ 表示激活函数， Ni是包括节点 i的节点i的邻域。
[0085]通过上述公式3可知， 本 实施例中， 编码器在第k层生成的节点i的表示通过可训练
参数、 σ 激活函数、 第k ‑1个编码器层生成的节点j的表示以及第k个编码器层中相 邻节点j与
节点i的归一化第一相关系数/注意系数计算得到 。
[0086]步骤S404， 通过解码器层 计算所述数据集中的各样本节点与 对应的相邻节点之间
的相关性， 得到各样本节点的第二相关系数。
[0087]具体地， 通过使用与编码器相同层数的解码器， 每个解码器层试 图逆转其相应编
码器层的过程。 换句话说， 每个解码器层根据节点的相关性利用其邻居的表示来重建节点
的表示。 在训练GATE模 型时， 将预先采集的训练集输入GATE模型中进 行训练， 将各样 本节点
与对应的相邻节点输入至解码器层中， 解码 器层通过计算各样本节点与对应的相 邻节点之
间的相关性， 得到各样本节点与对应的相邻节点之间的第二相关系数。
[0088]进一步地， 步骤S404， 通过解码器层计算所述数据集中的各样本节点与对应的相
邻节点之间的相关性， 得到各样本节点的第二相关系数， 可以包括：
采用具有节点间共享参数的自我关注机制， 通过解码器层计算得到各样本节点的
第二相关系数；
对各样本节点的第二相关系数进行归一 化处理。
[0089]更为具体地， 采用具有节点间共享参数的自我关注机制， 通过解码器层计算该样
本节点与其相邻节点之间的相关系数， 并称作第二相关系数。 在第k个解码器层中， 相邻节
点j和节点i的相关性计算如下公式4所示， 得到第k个解码 器层中相 邻节点j与节点i的第二
相关系数/注 意系数， 然后采用Softmax函数对第k个解码器层生 成的节点i的第二相关系数
进行归一 化处理， 如下公式5所示：
（4）
（5）说　明　书 12/19 页
16
CN 114841296 A
16其中，
 是第k个解码器层中相邻节点j与节点i的第二相关系数/注意系数，
是可训练参数， σ 表示激活函数， Sigmoid表示Sigmoid函 数，
 是第k
个解码器层生成的节点i的表示，
 是第k个解码器层生成的节点j的表示；
 是第k
个解码器层中相 邻节点j与节点i的归一化第二相关系数/注 意系数， Ni是包括节 点i的节点
i的邻域， l是节点 i的邻域内任一相邻节点。
[0090]通过上述公式4和5可知， 在本实施例中， 解码器层采用了具有节点间共享参数的
自我关注机制计算得到相邻节点j和节点i的第二相关系数， 并为了使节点i的相关系数具
有可比性， 对第二相关系数进行了归一 化处理。
[0091]步骤S405， 基于解码器层得到的第二相关系数， 通过解码器层采用 邻居的表示来
重建节点的表示， 可以包括：
结合归一化处理后的第二相关系数， 通过将对应编码器层输出的样本节点的表示
作为解码器层的输入， 采用对应的解码器层重建各样本节点的表示。
[0092]具体地， 通过将对应编码器层输出的样本节点的表示作为解码器层的输入 （即
，∀i∈{1， 2， .， N}） ， 解码器在第 k‑1层重建节点i的表示， 具体计算过程如下
公式6所示：
（6）
其中，
 是第k‑1个解码器层重建的节点i的表示；
 是第k个解码器层
生成的节点j的表示， 其中， 可将编码器层的输出视为解码器层的输入 （即
 ，
∀i∈{1， 2， .， N}） ；
 是第k个解码器层中相邻节点j与 节点i的归一化第二相关系数/注
意系数；
 是可训练参数； σ 表示激活函数； N i是包括节点 i的节点i的邻域。
[0093]通过上述公式6可知， 本 实施例中， 解码器在第k ‑1层重建的节点i的表示通过可训
练参数、 σ 激活函数、 第k个解码 器层生成的节点j的表 示及第k个解码 器层中相 邻节点j与节
点i的归一化第二相关系数/注意系数计算得到 。
[0094]步骤S406， 结合重建后的各样本节点的表示与预设的目标损失函数进行模型训
练， 最小化节 点特征和图结构的重 建损失， 输出重 建后的节 点特征矩阵， 得到训练后的GATE
模型。
[0095]具体地， 结合重建后的各样本节点的表示与预设的目标损失函数进行可训练参数
的循环迭代， 以此最小化节点特征和图结构的重建损失， 当节点特征和图结构的重建损失
在预设损失值内， 得到确定的可训练参数， 即GATE模 型收敛， 通过收敛后的GATE模 型输出重说　明　书 13/19 页
17
CN 114841296 A
17建后的节点特 征矩阵， 并得到训练后的GATE模型。
[0096]进一步地， 参照图6， 图6为本申请实施例中结合重建后的各样本节点的表示与预
设的目标损失函数进行模型训练， 最小化节点特征和图结构的重建损失， 输出重建后的节
点特征矩阵， 得到训练后的GATE模 型的具体流程示意图。 上述步骤S406， 结合重 建后的各样
本节点的表示与预设的目标损失函数进行模型训练， 最小化节点特征和图结构的重建损
失， 输出重建后的节点特 征矩阵， 得到训练后的GATE模型， 可以包括：
步骤S4061， 计算各样本节点重建后的节点表示与初始 节点表示之间的差值。
[0097]具体地， 为将节点特征的重建损失最小化， 采用节点表示作为节点特征， 计算各样
本节点在重建后的节点表示与初始 节点表示之间的差值， 具体的计算如公式7 所示：
（7）
其中， N是图结构中的节点数， xi是节点i的初始节点特征 （其中， hi(0) = xi)，
是
节点i的重建节点特 征(其中，
 )。
[0098]步骤S4062， 计算重建后的各样本节点与对应的相邻节点之间的表示相似。
[0099]具体地， 由于特征相似的可能性， 图中两个节点之间没有边不一定意味着不相似。
换句话说， 两个节点间没有联系/边， 并不代表两个节 点之间没有相似性。 因此， 可通过使用
各样本节点与对应的相邻节点之间的表示相似来最小化图结构的重 建损失。 具体的该样本
节点与对应的相邻节点之间的表示相似通过如下公式8所示 来计算：
（8）
其中， N是图结构中的节点数， Ni是包 括节点i的节点i的邻域， hi是节点i的表示， hj
是相邻节点j的表示。
[0100]步骤S4063， 将所述差值与所述表示相似代入预设的目标损失函数， 计算得到节点
特征和图结构的重建损失。
[0101] 具体地， 结合各样本节点重建后的节点表示与初始节点表示之间的差值， 和重建
后的各样本节点与对应的相邻节点之 间的表示相似， 将所述差值和所述表示相似代入预设
的目标损失函数中， 具体目标损失函数的形式如公式9所示， 计算得到节点特征和图结构的
重建损失：
（9）
其中， Loss为节点特征和图结构的重建损失， xi是节点i的初始节点特征(其中， hi
(0) = xi)，
是节点i的重建节点特征(其中，
 )， hi是节点i的表示， hj是相邻说　明　书 14/19 页
18
CN 114841296 A
18节点j的表示， N是图结构中的节点数， Ni是包括节点i的节点i的邻域， λ是调整图结构重建
损失的参数。
[0102] 步骤S4064， 将所述节点特征和图结构的重建损失回传到GATE模型， 对编码器层和
解码器层的可训练参数进行更新； 并返回执行步骤； 通过编码器层计算所述数据集中的各
样本节点与对应的相邻节点之间的相关性， 得到各样本节点的第一相关系数。
[0103] 具体地， 将通过计算得到的节点特征和图结构的重建损失回传到GATE模型中， 对
编码器层和解码器层中的可训练参数进行更新， 并返回执行步骤， 重新按照步骤S402至步
骤S406对GATE模型进行训练， 输出新的节点特 征和图结构的重建损失。
[0104] 步骤S4065， 以此循环， 进行参数迭代， 最小化节点特征和图结构的重建损失， 直到
所述GATE模型收敛， 终止训练， 输出重建后的节点特 征矩阵， 得到训练后的GATE模型。
[0105] 具体地， 对步骤S4064进行循环， 不断迭代更新可训练参数， 直到节点特征和图结
构的重建损失在预设损失值范围内， 即认为得到最小化的节点特征和图结构的重建损失，
此时得到确定的可训练参数， 即GATE模型收敛， 可终止可训练参数的迭代。 同时， 通过训练
好的GATE模型输出重建后的节点特 征矩阵。
[0106] 通过上述公式7、 8和9可知， 本实施例中， 节点特征和图结构的重建损失通过结合
各样本节点重 建后的节点表示与初始节点表示之间的差值， 以及不同权重的重 建后的各样
本节点与对应的相邻节点之间的表示相似， 来最小化节点特征和图结构的重建损失， 以此
训练GATE模型。
[0107] 进一步地， 参照图7， 图7为本申请实施例中输出重建后的节点特征矩阵的具体流
程示意图。 上述输出重建后的节点特 征矩阵的步骤可以包括：
步骤S4071， 根据编码器层得到的第一相关系数， 获取所述编码器层的第一注意力
矩阵；
具体地， 在第k个编码器层中获得该层的第一注意力矩阵， 其中， 第一注意力矩阵
向量通过该编码器层得到的归一化第一相关系 数获得， 即Cij(k)=αij(k)， 如果节点i和节点j
之间有边， 则Cij(k)=0。 具体编码器层的第一注意力矩阵的计算方式如下公式10、 11和12所
示：
（10）
（11）
（12）
其中， C(k)是第k个编码器层中的第一注意力矩阵； A是邻接矩阵； H  (k‑1)是第k‑1个
编码器层生成的节点表示矩阵； W、 vs、 vr是可训练参数； ⊙是同或运算符， 表示为两个输入变
量值相同时输出为1； σ 表示激活函数； Sigmoid表示Sigmoid函数； Softmax表示Softmax函
数； Ms(k)和Mr(k)分别为两个指代函数。
[0108] 步骤S4072， 根据所述第一注意力矩阵， 通过所述编码器层 输出各样本节点的节点
表示矩阵；说　明　书 15/19 页
19
CN 114841296 A
19具体地， 通过考虑H(0)=X， 其中， X是节点特征矩阵， 在第k个编码器层中生成第k层
的节点表示矩阵， 其中， 第k层的各样本节点的节点表示矩阵计算方式如下公式13所示：
（13）
其中， H (k)是第k个编码器层生成的各样本节点的节点表示矩阵， H  (k‑1)是第k‑1个
编码器层生成的各样 本节点的节 点表示矩阵， C(k)是第k个编码 器层中的第一注 意力矩阵， W
是可训练参数， σ 表示激活函数。
[0109] 步骤S4073， 根据解码器层得到的第二相关系数， 获取所述解码器层的第二注意力
矩阵；
具体地， 在第k个解码器层中获得该层的第二注意力矩阵， 其中， 第二注意力矩阵
向量通过该解码器层得到的归一化第二相关系数获得， 即
 ， 如果节点i和节点
j之间有边， 则
 。 具体解码器层的第二注意力矩阵的计算方式如下公式14、 15和16
所示：
（14）
（15）
（16）
其中，
 是第k个解码 器层中的第二注 意力矩阵； A是邻接矩阵；
 是第k个
解码器层生成的各样本节点的节点特征矩阵； ⊙是同或运算符， 表示为两个输入变量值相
同时输出为1；
 是可训练参数； σ 表示激活函数； Sigmoid表示Sigmoid函
数； Softmax表示Softmax函数，
 和
分别为两个指代函数。
[0110]步骤S4074， 结合各样本节点的节点表示矩阵和解码器层的第二注意力矩阵， 通过
解码器层输出重建后的节点特 征矩阵。
[0111]具体地， 通过考虑
 ， 其中， L是解码器层的层数， H  (L)是第L层编码器
层生成的各样本节点的节点表示矩阵， 即， 将编码器层输出 的各样本节点的节点表示矩阵
作为对应解码器层的输入， 在第k个解码器层中重建第k ‑1层的各样本节点的节点特征矩
阵， 具体的第k ‑1层的节点表示矩阵的计算方式如下公式17 所示：
（17）说　明　书 16/19 页
20
CN 114841296 A
20其中，
 是由第k个解码器层重建的第k ‑1层的各样本节点的节点特征矩
阵，
 是第k个解码器层生成的各样本节点的节点特征矩阵，
 是第k个解码器层中
的第二注意力矩阵，
 是可训练参数， σ 表示激活函数。
[0112]通过上述公式10 ‑17可知， 本 实施例中， 由于邻接矩阵A在实践中通常是稀疏的， 因
此利用稀疏矩阵操作， 如Softmax函数来处理图结构。 GATE模 型通过计算上述编码 器层操作
所对应的矩阵公 式 （如公式10‑13） 和解码 器层操作所对应的矩阵公式 （如公 式14‑17） ， 得到
重建后的节点特 征矩阵。
[0113]相比现有技术， 本实施例方案， 通过获取各设备间的连接关系， 所述各设备构成图
注意力网络 GAT网络的网络节 点； 根据所述各设备间的连接 关系， 得到所述各设备的邻接矩
阵； 对所述各设备进 行特征提取， 得到所述各设备的特征矩阵； 将所述邻接矩阵和特征矩阵
输入至预先训练好的图注意自编码器GATE模型中， 得到重建后的节点特征矩阵； 将所述重
建后的节点特征矩阵输入至聚类算法中对各网络节点进行聚类， 得到所述各设备 的类别。
基于本申请方案， 从相似通信行为的设备间具备较高相似度的规则出发， 构建了一个能将
设备按照相似性进行分类的设备聚类模型， 基于本申请构建的设备聚类模型， 在设备增加
的情况下， 无需更新全图的节点特征并对全图进行重新计算， 最后经过本申请方案实现设
备聚类的方法节省了大量计算空间。
[0114]参考图8， 图8为本申请设备聚类方法第五示例性实施例的整体流程示意图， 本实
施例的整体流程包括： 通过获取各设备间的连接 关系， 所述各设备构成图注意力网络 GAT网
络的网络节点； 根据所述各设备间的连接 关系， 得到所述各设备的邻接矩阵； 对 所述各设备
进行特征提取， 得到所述各设备 的特征矩阵； 将所述邻接矩阵和特征矩阵输入至预先训练
好的图注意自编码器GATE模型中， 得到重建后的节点特征矩阵； 将所述重建后的节点特征
矩阵输入至聚类算法中对各网络节点进行聚类， 得到所述各设备的类别。
[0115]进一步地， 参照图9， 图9为本申请实施例中将所述重建后的节点特征矩阵输入至
聚类算法中对各网络节点进行聚类， 得到所述各设备 的类别的具体流程示意图。 基于上述
图5所示的实施例， 在本实施例中， 上述步骤S 51， 将所述重 建后的节 点特征矩阵输入至聚类
算法中对各网络节点进行聚类， 得到所述各设备的类别， 可以包括：
步骤S511， 根据所述重建后的节点特 征矩阵， 采用肘部法则自适应选取 K值；
步骤S512， 根据所述K值， 采用K ‑means聚类算法对各网络节点进行聚类， 得到每个
所述设备的类别。
[0116]具体地， 本实施例中， 由于经GATE模型输出的重建后的节点特征矩阵不具备类别
标签， 因此事先无法知晓网络节点的分类数。 采用聚类算法对 各网络节点进行聚类， 首先需
要确定聚类的簇中心数量， 即K值。
[0117]本实施例中采用K ‑means聚类算法对各 网络节点进行聚类， 其中K值的选取采用肘
部法则来确 定。 肘部法则 是一种K‑means聚类的K值选择规则， 通过在K值增大过程中， 自适
应选取畸变程度的改善效果下降幅度最大的位置所对应的值 （即肘部） 作为K ‑means聚类性
能较好的K值。
[0118]然后， 根据肘部法则自适应选取的K值， 将重建后的节点特征矩阵输入至K ‑means说　明　书 17/19 页
21
CN 114841296 A
21聚类算法中对各网络节点进行聚类， 根据聚类结果， 得到每 个所述设备的类别。
[0119]通过肘部法则自适应选取K值， 可以提高K ‑means聚类算法的聚类性能， 应用此K ‑
means聚类算法对GATE模型输出的重建后的节点特征矩阵进行网络节点间的聚类， 能更好
地找出设备间的聚类关系。
[0120] 本实施例通过上述方案， 从相似通信行为的设备间具备较高相似度的规则出发，
构建了一个能将设备按照相似性进行分类的设备聚类模型， 基于本申请构建的设备聚类模
型， 在设备增加的情况下， 无需更新全图的节点特征并对全图进 行重新计算， 最后经过本申
请方案实现设备聚类的方法节省了大量计算空间。
[0121] 此外， 本申请实施例还提出一种设备聚类装置， 所述设备聚类装置包括：
关系获取模块， 用于获取各设备 间的连接关系， 所述各设备构成图注意力网络GAT
网络的网络节点；
邻接矩阵模块， 用于根据所述各设备间的连接关系， 得到所述各设备的邻接矩阵；
特征矩阵模块， 用于对所述各设备进行 特征提取， 得到所述各设备的特 征矩阵；
图注意力模块， 用于将 所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编
码器GATE模型中， 得到 重建后的节点特 征矩阵；
设备聚类模块， 用于将 所述重建后的节点特征矩阵输入至聚类算法中对各网络节
点进行聚类， 得到所述各设备的类别。
[0122]进一步地， 所述设备聚类装置还 包括：
模型训练模块， 用于基于堆叠的编码器层和解码器层训练得到所述GATE模型。
[0123] 本实施例实现设备聚类的原理及实施过程， 请参照上述各实施例， 在此不再一一
赘述。
[0124] 本实施例通过上述方案， 从相似通信行为的设备间具备较高相似度的规则出发，
构建了一个能将设备按照相似性进行分类的设备聚类模型， 基于本申请构建的设备聚类模
型， 在设备增加的情况下， 无需更新全图的节点特征并对全图进 行重新计算， 最后经过本申
请方案实现设备聚类的方法节省了大量计算空间。
[0125] 此外， 本申请实施例还提出一种终端 设备， 所述终端设备包括存储器、 处理器及存
储在所述存储器上并可在所述处理器上运行的设备聚类程序， 所述设备聚类程序被所述处
理器执行时实现如上 所述的设备聚类方法的步骤。
[0126] 由于本设备聚类程序被处理器执行时， 采用了前述所有实施例的全部技术方案，
因此至少具有前述所有实施例的全部技术方案所带来的所有有益效果， 在此不再一一赘
述。
[0127] 此外， 本申请实施例还提出一种计算机可读存储介质， 所述计算机可读存储介质
上存储有设备聚类程序， 所述设备聚类程序被处理器执行时实现如上所述的设备聚类方法
的步骤。
[0128] 由于本设备聚类程序被处理器执行时， 采用了前述所有实施例的全部技术方案，
因此至少具有前述所有实施例的全部技术方案所带来的所有有益效果， 在此不再一一赘
述。
[0129] 相比现有技术， 本申请实施例提出的设备聚类方法、 装置、 终端设备以及存储介
质， 通过获取各设备间的连接关系， 所述各设备构成图注意力网络GAT网络的网络节点； 根说　明　书 18/19 页
22
CN 114841296 A
22据所述各设备间的连接 关系， 得到所述各设备的邻接矩阵； 对 所述各设备进行特征提取， 得
到所述各设备的特征矩阵； 将所述邻接矩阵和特征矩阵输入至预先训练好的图注意自编码
器GATE模型中， 得到重建后的节点特征矩阵； 将所述重建后的节点特征矩阵输入至聚类算
法中对各网络节点进 行聚类， 得到所述各设备的类别。 基于本申请方案， 从相似通信行为的
设备间具备较高相似度的规则出发， 构建了一个能将设备按照相似性进 行分类的设备聚类
模型， 基于本申请构建的设备聚类模型， 在设备增加的情况下， 无需更新全图的节点特征并
对全图进行重新计算， 最后经 过本申请方案实现设备聚类的方法节省了大量计算空间。
[0130] 需要说明的是， 在本文中， 术语 “包括”、“包含”或者其任何其他变体意在涵盖 非排
他性的包含， 从而使得包括一系列要素 的过程、 方法、 物品或者系统不仅包括那些要素， 而
且还包括没有明确列出的其他要 素， 或者是还包括为这种过程、 方法、 物品或者系统所固有
的要素。 在没有 更多限制的情况下， 由语句 “包括一个 ……”限定的要 素， 并不排除在 包括该
要素的过程、 方法、 物品或者系统中还 存在另外的相同要素。
[0131] 上述本申请实施例序号仅 仅为了描述， 不代 表实施例的优劣。
[0132] 通过以上的实施方式的描述， 本领域的技术人员可以清楚地了解到上述实施例方
法可借助软件加必需的通用硬件平台的方式来实现， 当然也可以通过硬件， 但很多情况下
前者是更佳的实施方式。 基于这样的理解， 本申请的技术方案本质上或者说对现有技术做
出贡献的部 分可以以软件产品的形式体现出来， 该计算机软件产品存储在如上的一个存储
介质(如ROM/RAM、 磁碟、 光盘)中， 包括若干指令用以使得一台终端设备(可以是手机， 计算
机， 服务器， 被控终端， 或者网络设备等)执 行本申请每 个实施例的方法。
[0133]以上仅为本申请的优选实施例， 并非因此限制本申请的专利范围， 凡是利用本申
请说明书及附图内容所作的等效结构或等效流程变换， 或直接或间接运用在其他相关的技
术领域， 均同理包括在本申请的专利保护范围内。说　明　书 19/19 页
23
CN 114841296 A
23图1
图2说　明　书　附　图 1/5 页
24
CN 114841296 A
24图3
图4说　明　书　附　图 2/5 页
25
CN 114841296 A
25图5说　明　书　附　图 3/5 页
26
CN 114841296 A
26图6
图7说　明　书　附　图 4/5 页
27
CN 114841296 A
27图8
图9说　明　书　附　图 5/5 页
28
CN 114841296 A
28