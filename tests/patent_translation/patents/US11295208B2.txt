US011295208B2 
 ( 12 ) United States Patent Agrawal et al . ( 10 ) Patent No .: US 11,295,208 B2 
 ( 45 ) Date of Patent : Apr. 5 , 2022 
 ( 56 ) References Cited ( 54 ) ROBUST GRADIENT WEIGHT COMPRESSION SCHEMES FOR DEEP 
 LEARNING APPLICATIONS U.S. PATENT DOCUMENTS 
 5,926,804 A 7,088,856 B2 * 7/1999 Tufts et al . 
 8/2006 Lee ( 71 ) Applicant : International Business Machines Corporation , Armonk , NY ( US ) G06K 9/4652 
 348 / 404.1 
 ( Continued ) 
 FOREIGN PATENT DOCUMENTS ( 72 ) Inventors : Ankur Agrawal , White Plains , NY ( US ) ; Daniel Brand , Millwood , NY ( US ) ; Chia - Yu Chen , Westchester , NY ( US ) ; Jungwook Choi , Elmsford , NY ( US ) ; Kailash Gopalakrishnan , San 
 Jose , CA ( US ) CN 104598972 A 5/2015 
 OTHER PUBLICATIONS 
 ( 73 ) Assignee : INTERNATIONAL BUSINESS MACHINES CORPORATION , Armonk , NY ( US ) Model Accuracy and Runtime Tradeoff in Distributed Deep Learn ing Gupta et al . ( Year : 2015 ) . * 
 ( Continued ) 
 Primary Examiner Kakali Chaki 
 Assistant Examiner Ababacar Seck ( 74 ) Attorney , Agent , or Firm — Cantor Colburn LLP ; Stosch Sabo ( * ) Notice : Subject to any disclaimer , the term of this patent is extended or adjusted under 35 U.S.C. 154 ( b ) by 1157 days . 
 ( 21 ) Appl . No .: 15 / 830,170 
 ( 22 ) Filed : Dec. 4 , 2017 
 ( 65 ) Prior Publication Data 
 US 2019/0171935 A1 Jun . 6 , 2019 ( 57 ) ABSTRACT 
 Embodiments of the present invention provide a computer implemented method for adaptive residual gradient com pression for training of a deep learning neural network ( DNN ) . The method includes obtaining , by a first learner , a current gradient vector for a neural network layer of the DNN , in which the current gradient vector includes gradient weights of parameters of the neural network layer that are calculated from a mini - batch of training data . A current residue vector is generated that includes residual gradient weights for the mini - batch . A compressed current residue vector is generated based on dividing the residual gradient weights of the current residue vector into a plurality of bins of a uniform size and quantizing a subset of the residual gradient weights of one or more bins of the plurality of bins . The compressed current residue vector is then transmitted to a second learner of the plurality of learners or to a parameter ( 51 ) Int . Cl . GO6N 3/08 ( 2006.01 ) 
 GO6N 3/04 ( 2006.01 ) 
 ( 52 ) U.S. Ci . CPC G06N 3/08 ( 2013.01 ) ; G06N 3/04 ( 2013.01 ) 
 ( 58 ) Field of Classification Search CPC GOON 3/08 ; GO6N 3/04 ; GOON 3/0445 ; 
 GO6N 3/084 ; GO6N 3/0454 See application file for complete search history . server . 
 25 Claims , 8 Drawing Sheets 
 Convolution Fully connected 
 W : ? 
 ? : 
 LO ( Input ) 
 512x512 12 13 14 
 128x128 64x64 32x32 256x256 ES F6 
 450 ( Output ) 
 400 420 430 440 460 US 11,295,208 B2 
 Page 2 
 ( 56 ) References Cited 
 U.S. PATENT DOCUMENTS 
 9,235,799 B2 9,626,621 B2 
 2017/0098171 A1 2018/0075347 A1 * 
 2018/0107926 A1 * 
 2018/0144242 A1 * 2018/0268283 A1 * 1/2016 Yu et al . 4/2017 Dognin et al . 4/2017 Kumar et al . 3/2018 Alistarh 4/2018 Choi 
 5/2018 Simard 9/2018 Gilad - Bachrach GOON 3/084 
 GOON 3/063 
 GOON 3/0454 G06F 17/11 
 OTHER PUBLICATIONS 
 A Reliable Effective Terascale Linear Learning System Gupta et al . 
 ( Year : 2014 ) . * Stochastic Gradient Made Stable : A Manifold Propagation Approach for Large - Scale Optimization Yadong Mu , Member , IEEE , Wei Liu , Member , IEEE , and Wei Fan , Member , IEEE ( Year : 2016 ) . * Deep Residual Learning for Image Recognition He et al . ( Year : 
 2016 ) . * Communication Quantization for Data - parallel Training of Deep Neural Networks Dryden et al . ( Year : 2016 ) . * Bagherinezhad et al .; “ LCNN : Lookup - based Convolutional Neural Network ” ; University of Washington , Allen Institute for AI ; Nov. 2016 ; 10 pages . Chen et al .; “ Compressing Convolutional Neural Networks in the Frequency Domain ” ; Cornell University Library ; Jun . 14 , 2015 ; 10 pages . Han et al . , “ Learning both Weights and Connections for Efficient Neural Networks ” ; Advances in Neural Information Processing Systems 28 ( NIPS 2015 ) ; 9 pages . Kadetotad et al . , “ Efficient Memory Compression in Deep Neural Networks Using Coarse - Grain Sparsification for Speech Applica tions ” ; Proceedings of the 35th International Conference on Computer Aided Design , Article No. 78 ; Nov. 7-10 , 2016 ; 8 pages . Kim et al .; “ Audio Computing in the Wild : Frameworks for Big Data and Small Computers ” Dissertation - University of Illinois at Urbana - Champaign , 2016 ; 130 pages . Lascorz et al . , “ Tartan : Accelerating Fully - Connected and Convo lutional Layers in Deep Learning Networks By Exploiting Numeri cal Precision Variability ” ; Electrical and Computer Engineering University of Toronto , Conference paper at ICLR 2017 ; 14 pages . Li et al . , Lecture 4 : Backpropagation and Neural Networks ; Lecture 4-1 ; Apr. 13 , 2017 ; 100 pages . Li et al ; “ Lecture 3 : Loss Functionsand Optimization ” ; Lecture 3-1 ; Apr. 11 , 2017 ; 85 pages . Mell et al .; “ The NIST Definition of Cloud Computing Recommendations of the National Institute of Standards and Tech nology ” ; U.S. Department of Commerce ; Sep. 2011 ; 7 pages . Meng et al .; “ Two - Bit Networks for Deep Learning on Resource Constrained Embedded Devices ” ; Cornell University Library ; Jan. 
 2 , 2017 ; 2 pages . Parashar et al . , " SCNN : An Accelerator for Compressed - sparse Convolutional Neural Networks ” ; ISCA ’17 , Jun . 24-28 , 2017 , Toronto , ON , Canada ; 14 pages . Seide et al .; “ 1 -Bit Stochastic Gradient Descent and its Application to Data - Parallel Distributed Training of Speech DNNs " ; Institute of Microelectronics , Tsinghua University , Beijing CN ; Interspeech 2014 ; 5 pages . PCT International Search Report and Written Opinion ; Application No. PCT / IB2018 / 059516 ; dated Mar. 27 , 2019 ; 9 pages . Strom , " Scalable distributed DNN training using commodity GPU cloud computing , ” Sixteenth Annual Conference of the Interna tional Speech Communication Association , 2015 , pp . 1488-1492 . UK Examination Report dated Aug. 5 , 2020 in GB2009717.6 ( 6 pages ) 
 * cited by examiner U.S. Patent Apr. 5 , 2022 Sheet 1 of 8 US 11,295,208 B2 
 OS 10 54 
 54 N 
 FIG . 1 
 8 54 han 54 
 a U.S. Patent 
 Z --- 
 91 **** .. * 
 92 93 94 95 --------- 
 - ... ---- 
 --------- 96 
 a . 
 ******** - 
 *** 
 Workloads 90 Apr. 5 , 2022 
 81 82 83 84 85 
 Management 
 08 F Sheet 2 of 8 
 71 72 73 74 75 
 Virtualization 
 70 10 61 62 63 64 65 66 67 68 
 Hardware and Software 
 :: 60 FIG . 2 US 11,295,208 B2 U.S. Patent Apr. 5 , 2022 Sheet 3 of 8 US 11,295,208 B2 
 300 
 ***** 
 PROCESSOR -302 
 MAIN MEMORY 
 308 310 
 DISPLAY INTERFACE DISPLAY UNT 
 306 
 SECONDARY MEMORY -312 COMMUNICATION INFRASTRUCTURE ( BUS ) DISK DRIVE 
 REMOVABLE STORAGE DRIVE 318 / 
 REMOVABLE STORAGE UNIT 316 
 MAN INTERFACE REMOVABLE STORAGE 320 322 
 COMMUNICATION INTERFACE COMMUNICATION PATH 
 324 
 326 
 FIG . 3 U.S. Patent 
Convolution Fully connected Apr. 5 , 2022 
 ? 
 W O 
 25 ## * * * 2 Sheet 4 of 8 
 LO ( Input ) 512x512 L2 L3 128x128 64x64 32x32 
 256x256 F5 F6 450 ( Output ) 
 400 420 430 440 460 
 410 FIG . 4 US 11,295,208 B2 500 U.S. Patent 
 Input Maps 510 * Convolution Kernels 520 + Bias 530 = > Output Maps 530 M Output Maps 
 N input maps N filters k pixels 
 k pixels Filter Bank 1 Apr. 5 , 2022 
 n pixels Filter Bank 2 Filter Bank Sheet 5 of 8 
 a The input is a set of N maps , Each map is al matrix of size ( nx 
 n ) pixels divided into pieces of 
 size ( k xk ) . n - k + 1 pixels 
 3 The output is a set of M 
 maps , Each map is a matrix of size 
 ( n - k + 1 xn - k + 1 ) 
 Filter Bank M 
 INPUT Trainable Parameters OUTPUT US 11,295,208 B2 
 FIG . 5 U.S. Patent 
 600 
 Learner Processing System 610 Apr. 5 , 2022 
 Learner Processing System 630 Network 650 Learner Processing System 620 Sheet 6 of 8 
 Learner Processing System 640 FIG . 6 US 11,295,208 B2 Randomly initialize network weights 702 700 U.S. Patent 
 Receive training data 704 
 Pick a minibatch 706 
 Perform forward pass through the network 708 Compute error using the correct outputs 710 Apr. 5 , 2022 
 Perform backward pass through the network 712 Obtain a current gradient vector 714 
 Generate a compressed current residue vector 716 Exchange the compressed vectors 718 
 Decompress the compressed vectors 720 Sheet 7 of 8 
 Average local gradients 722 
 Update the network weights 724 
 No is epoch finished ? 726 Yes 
 FIG . 7 No Yes US 11,295,208 B2 
 Is training done ? 728 STOP 716 U.S. Patent 
GENERATINGA SCALLED CURRENT RESIDUE VECTOR THAT INCLUDES SCALED RESIDUAL GRADIENT WEIGHTS FOR A GIVEN MINI BATCH 802 DIVIDING THE RESIDUAL GRADIENT WEIGHTS OF THE CURRENT RESIDUE VECTOR INTO A PLURALITY OF BINS OF A UNIFORM SIZE 804 Apr. 5 , 2022 
 IDENTIFYING ALOCAL MAXIMUM FOR EACH GIVEN RESIDUAL GRADIETN WEIGHT OF EACH GIVEN BIN 806 DETERMINING FOR EACH GIVEN RESIDUAL GRADIETN WEIGHT OF EACH GIVEN BIN WHETHER THE GIVEN RESIDUAL GRADIETN WEIGHT HAS A CORRESPONDING SCALED RESIDUAL GRADIENT WEIGHT THAT EXCEEDS THE LOCAL MAXIMUM OF THE GIVEN BIN 808 Sheet 8 of 8 
 GENERATING A QUANTIZING VALUE FOR THE GIVE RESIDUAL GRADIENT WEIGHT AND UPDATING THE CURRENT RESIDUE VECTOR BY SUBSTITUTING THE GIVEN RESIDUAL GRADIENT WEIGHT OF THE CURRENT RESIDUE VECTOR WITH THE QUANTIZED VALUE 810 FIG . 8 US 11,295,208 B2 5 
 a 
 a 
 a US 11,295,208 B2 
 1 2 
 ROBUST GRADIENT WEIGHT and calculating a local maximum of each bin , in which the COMPRESSION SCHEMES FOR DEEP uniform size of the bins is a hyper - parameter of the DNN . 
 LEARNING APPLICATIONS The compressed current residue vector is then transmitted by the processor to a second learner of the plurality of learners . 
 BACKGROUND Embodiments of the invention provide a computer pro gram product for adaptive residual gradient compression for The present invention generally relates to training of deep training of a DNN , the computer program product having a learning networks , and more specifically , to robust residual computer readable storage medium having program instruc gradient compression schemes for deep machine learning tions embodied therewith . The program instructions are applications . 10 executable by a processor of at least a first learner of a The phrase “ machine learning ” broadly describes a func- plurality of learners to cause the first learner to perform a tion of an electronic system that is capable of learning from method . A non - limiting example of the method includes data . A machine learning system , engine , or module can obtaining a current gradient vector for a neural network include a machine learning algorithm that can be trained , layer of the DNN , in which the current gradient vector such as in an external cloud environment , to learn functional 15 includes gradient weights of parameters of the neural net relationships between inputs and outputs , wherein the func- work layer that are calculated from a mini - batch of training tional relationships are currently unknown . data . A current residue vector is generated that includes residual gradient weights for the mini - batch , in which gen SUMMARY erating the current residue vector includes the summation of 20 a prior residue vector and the current gradient vector . A Embodiments of the present invention provide a com- compressed current residue vector is generated , in which the puter - implemented method for adaptive residual gradient compressed current residue vector is generated based at least compression for training of a deep learning neural network in part on dividing the residual gradient weights of the ( DNN ) . A non - limiting example of the method includes current residue vector into a plurality of bins of a uniform obtaining , by a processor of a first learner of a plurality of 25 size and quantizing a subset of the residual gradient weights learners , a current gradient vector for a neural network layer of one or more bins of the plurality of bins . The quantizing of the DNN , in which the current gradient vector includes of the subset of the residual gradient weights is based at least gradient weights of parameters of the neural network layer in part on calculating a scaling parameter for the mini - batch that are calculated from a mini - batch of training data . A and calculating a local maximum of each bin , in which the current residue vector is generated that includes residual 30 uniform size of the bins is a hyper - parameter of the DNN . gradient weights for the mini - batch , in which generating the The compressed current residue vector is then transmitted to current residue vector includes the summation of a prior a second learner of the plurality of learners . residue vector and the curre gradient vector . A compressed Embodiments of the present invention provide a com current residue vector is generated , in which the compressed puter - implemented method for training a DNN via adaptive current residue vector is generated based at least in part on 35 residual gradient compression . A non - limiting example of dividing the residual gradient weights of the current residue the method includes receiving , by a system having a plu vector into a plurality of bins of a uniform size and quan- rality of learners , training data for training of the DNN using tizing a subset of the residual gradient weights of one or one or more neural network layers . A current gradient vector more bins of the plurality of bins . The quantizing of the for a neural network layer is generated at each learner of the subset of the residual gradient weights is based at least in 40 plurality of learners from a mini - batch of the training data , part on calculating a scaling parameter for the mini - batch in which the current gradient vector includes gradient and calculating a local maximum of each bin , in which the weights of parameters of the neural network layer that are uniform size of the bins is a hyper - parameter of the DNN . calculated from a mini - batch of training data . A current The compressed current residue vector is then transmitted to residue vector is generated at each learner of the plurality of second learner of the plurality of learners . 45 learners from the mini - batch , in which generating the current Embodiments of the present invention provide a system residue vector includes summing a prior residue vector and for adaptive residual gradient compression for training of a the current gradient vector . A compressed current residue DNN . In some embodiments of the present invention , the vector is generated at each leaner of the plurality of learners , system includes a plurality of learners , in which at least a in which the compressed current residue vector is generated first learner of the plurality of learners is configured to 50 based at least in part on dividing the residual gradient perform a method . A non - limiting example of the method weights of the current residue vector into a plurality of bins includes obtaining a current gradient vector for a neural of a uniform size and quantizing a subset of the residual network layer of the DNN , in which the current gradient gradient weights of one or more bins of the plurality of bins . vector includes gradient weights of parameters of the neural The quantizing of the subset of the residual gradient weights network layer that are calculated from a mini - batch of 55 is based at least in part on calculating a scaling parameter for training data . A current residue vector that includes residual the mini - batch and calculating a local maximum of each bin , gradient weights for the mini - batch is generated , in which in which the uniform size of the bins is a hyper - parameter of generating the current residue vector includes summing a the DNN . The compressed current residue vectors are then prior residue vector and the current gradient vector . A exchanged among the plurality of learners . The compressed compressed current residue vector is generated , in which the 60 current residue vectors are decompressed at each of the compressed current residue vector is generated based at least plurality of learners . The gradient weights of the parameters in part on dividing the residual gradient weights of the of the neural network layer are then updated at each of the current residue vector into a plurality of bins of a uniform plurality of learners . size and quantizing a subset of the residual gradient weights Embodiments of the present invention provide a system of one or more bins of the plurality of bins . The quantizing 65 for training a deep learning neural network ( DNN ) via of the subset of the residual gradient weights is based at least adaptive residual gradient compression . In some embodi in part on calculating a scaling parameter for the mini - batch ments of the present invention , the system includes a plu a 
 a 
 a 
 ? 5 
 30 US 11,295,208 B2 
 3 4 
 rality of learners , in which the system is configured to ing of a deep learning neural network in accordance with one perform a method . A non - limiting example of the method or more embodiments of the present invention ; includes receiving , by the system , training data for training FIG . 7 depicts an example flowchart for training a neural 
 of the DNN using one or more neural network layers . A network in accordance with one or more embodiments of the current gradient vector for a neural network layer is gener- present invention ; and ated at each learner of the plurality of learners from a FIG . 8 depicts a flow diagram illustrating an example mini - batch of the training data , in which the current gradient methodology for generating a compressed current residue vector includes gradient weights of parameters of the neural vector in accordance with one or more embodiments of the network layer that are calculated from a mini - batch of present invention . training data . A current residue vector is generated at each 10 The diagrams depicted herein are illustrative . There can learner of the plurality of learners from the mini - batch , in be many variations to the diagram or the operations which generating the current residue vector includes the described therein without departing from the spirit of the summation of a prior residue vector and the current gradient vector . A compressed current residue vector is generated at invention . For instance , the actions can be performed in a 
 each leaner of the plurality of learners , in which the com- 15 differing order or actions can be added , deleted or modified . 
 pressed current residue vector is generated based at least in Also , the term " coupled ” and variations thereof describes 
 part on dividing the residual gradient weights of the current having a communications path between two elements and 
 residue vector into a plurality of bins of a uniform size and does not imply a direct connection between the elements 
 quantizing a subset of the residual gradient weights of one with no intervening elements / connections between them . All or more bins of the plurality of bins . The quantizing of the 20 of these variations are considered a part of the specification . subset of the residual gradient weights is based at least in In the accompanying figures and following detailed part on calculating a scaling parameter for the mini - batch description of the disclosed embodiments , the various ele and calculating a local maximum of each bin , in which the ments illustrated in the figures are provided with two or three uniform size of the bins is a hyper - parameter of the DNN . digit reference numbers . With minor exceptions , the leftmost The compressed current residue vectors are then exchanged 25 digit ( s ) of each reference number correspond to the figure in among the plurality of learners . The compressed current which its element is first illustrated . residue vectors are decompressed at each of the plurality of learners . The gradient weights of the parameters of the DETAILED DESCRIPTION neural network layer are then updated at each of the plurality 
 of learners . Various embodiments of the invention are described Additional technical features and benefits are realized herein with reference to the related drawings . Alternative through the techniques of the present invention . Embodi embodiments of the invention can be devised without ments and aspects of the invention are described in detail departing from the scope of this invention . Various connec herein and are considered a part of the claimed subject tions and positional relationships ( e.g. , over , below , adja matter . For a better understanding , refer to the detailed 35 cent , etc. ) are set forth between elements in the following description and to the drawings . description and in the drawings . These connections and / or This compression scheme could integrate with different training scheme ( time - domain ) and applied in different positional relationships , unless specified otherwise , can be 
 layers ( space - domain ) . For example , the compression direct or indirect , and the present invention is not intended scheme is not applied first few epochs or the last and first 40 to be limiting in this respect . Accordingly , a coupling of 
 layers of neural networks . entities can refer to either a direct or an indirect coupling , and a positional relationship between entities can be a direct 
 BRIEF DESCRIPTION OF THE DRAWINGS or indirect positional relationship . Moreover , the various tasks and process steps described herein can be incorporated The specifics of the exclusive rights described herein are 45 into a more comprehensive procedure or process having particularly pointed out and distinctly claimed in the claims additional steps or functionality not described in detail at the conclusion of the specification . The foregoing and herein . other features and advantages of the embodiments of the The following definitions and abbreviations are to be used invention are apparent from the following detailed descrip- for the interpretation of the claims and the specification . As tion taken in conjunction with the accompanying drawings 50 used herein , the terms " comprises , " " comprising , " 
 in which : " includes , ” “ including , " " has , ” “ having , " " contains ” or FIG . 1 depicts a cloud computing environment according " containing , ” or any other variation thereof , are intended to to one or more embodiments of the present invention ; cover a non - exclusive inclusion . For example , a composi FIG . 2 depicts abstraction model layers according to one tion , a mixture , a process , a method , an article , or an or more embodiments of the present invention ; 55 apparatus that comprises a list of elements is not necessarily FIG . 3 depicts an exemplary computer system capable of limited to only those elements but can include other ele implementing one or more embodiments of the present ments not expressly listed or inherent to such composition , invention ; mixture , process , method , article , or apparatus . FIG . 4 depicts a simplified block diagram of a convolu- Additionally , the term " exemplary ” is used herein to mean tional neural network in accordance with one or more 60 “ serving as an example , instance or illustration . ” Any embodiments of the present invention ; embodiment or design described herein as “ exemplary ” is FIG . 5 depicts an example convolution layer of a neural not necessarily to be construed as preferred or advantageous network that is being trained using training data in accor- over other embodiments or designs . The terms at least one ” dance with one or more embodiments of the present inven- and “ one or more ” may be understood to include any integer tion ; 65 number greater than or equal to one , i.e. one , two , three , FIG . 6 depicts a block diagram of an environment for four , etc. The terms “ a plurality ” may be understood to performing adaptive residual gradient compression for train- include any integer number greater than or equal to two , i.e. > 
 a 
 a a 
 ? ( 4 
 a US 11,295,208 B2 
 5 6 
 two , three , four , five , etc. The term “ connection ” may a cloud infrastructure . The applications are accessible from include both an indirect “ connection ” and a direct “ connec- various client devices through a thin client interface such as tion . " a web browser ( e.g. , web - based e - mail ) . The consumer does The terms " about , " " substantially , " " approximately , " and not manage or control the underlying cloud infrastructure variations thereof , are intended to include the degree of error 5 including network , servers , operating systems , storage , or associated with measurement of the particular quantity even individual application capabilities , with the possible based upon the equipment available at the time of filing the exception of limited user - specific application configuration application . For example , " about " can include a range of settings . + 8 % or 5 % , or 2 % of a given value . Platform as a Service ( PaaS ) : the capability provided to For the sake of brevity , conventional techniques related to 10 the consumer is to deploy onto the cloud infrastructure making and using aspects of the invention may or may not consumer - created or acquired applications created using be described in detail herein . In particular , various aspects of programming languages and tools supported by the provider . computing systems and specific computer programs to The consumer does not manage or control the underlying implement the various technical features described herein cloud infrastructure including networks , servers , operating are well known . Accordingly , in the interest of brevity , many 15 systems , or storage , but has control over the deployed conventional implementation details are only mentioned applications and possibly application hosting environment briefly herein or are omitted entirely without providing the configurations . well - known system and / or process details . Infrastructure as a Service ( IaaS ) : the capability provided It is to be understood that although this disclosure to the consumer is to provision processing , storage , net includes a detailed description on cloud computing , imple- 20 works , and other fundamental computing resources where mentation of the teachings recited herein are not limited to the consumer is able to deploy and run arbitrary software , a cloud computing environment . Rather , embodiments of the which can include operating systems and applications . The present invention are capable of being implemented in consumer does not manage or control the underlying cloud conjunction with any other type of computing environment infrastructure but has control over operating systems , stor now known or later developed . 25 age , deployed applications , and possibly limited control of Cloud computing is a model of service delivery for select networking components ( e.g. , host firewalls ) . enabling convenient , on - demand network access to a shared Deployment Models are as follows : pool of configurable computing resources ( e.g. , networks , Private cloud : the cloud infrastructure is operated solely network bandwidth , servers , processing , memory , storage , for an organization . It may be managed by the organization applications , virtual machines , and services ) that can be 30 or a third party and may exist on - premises or off - premises . rapidly provisioned and released with minimal management Community cloud : the cloud infrastructure is shared by effort or interaction with a provider of the service . This cloud several organizations and supports a specific community that model may include at least five characteristics , at least three has shared concerns ( e.g. , mission , security requirements , service models , and at least four deployment models . policy , and compliance considerations ) . It may be managed 
 Characteristics are as follows : 35 by the organizations or a third party and may exist on On - demand self - service : a cloud consumer can unilater- premises or off - premises . ally provision computing capabilities , such as server time Public cloud : the cloud infrastructure is made available to and network storage , as needed automatically without the general public or a large industry group and is owned by requiring human interaction with the service's provider . an organization selling cloud services . Broad network access : capabilities are available over a 40 Hybrid cloud : the cloud infrastructure is a composition of network and accessed through standard mechanisms that two or more clouds ( private , community , or public ) that promote use by heterogeneous thin or thick client platforms remain unique entities but are bound together by standard ( e.g. , mobile phones , laptops , and PDAs ) . ized or proprietary technology that enables data and appli Resource pooling : the provider's computing resources are cation portability ( e.g. , cloud bursting for load balancing pooled to serve multiple consumers using a multi - tenant 45 between clouds ) . model , with different physical and virtual resources dynami- A cloud computing environment is service oriented with cally assigned and reassigned according to demand . There is a focus on statelessness , low coupling , modularity , and a sense of location independence in that the consumer semantic interoperability . At the heart of cloud computing is generally has no control or knowledge over the exact an infrastructure that includes a network of interconnected location of the provided resources but may be able to specify 50 nodes . location at a higher level of abstraction ( e.g. , country , state , Referring now to FIG . 1 , illustrative cloud computing 
 or datacenter ) . environment 50 is depicted . As shown , cloud computing Rapid elasticity : capabilities can be rapidly and elastically environment 50 includes one or more cloud computing provisioned , in some cases automatically , to quickly scale nodes 10 with which local computing devices used by cloud out and rapidly released to quickly scale in . To the consumer , 55 consumers , such as , for example , personal digital assistant the capabilities available for provisioning often appear to be ( PDA ) or cellular telephone 54A , desktop computer 54B , unlimited and can be purchased in any quantity at any time . laptop computer 54C , and / or automobile computer system Measured service : cloud systems automatically control 54N may communicate . Nodes 10 may communicate with and optimize resource use by leveraging a metering capa- one another . They may be grouped ( not shown ) physically or bility at some level of abstraction appropriate to the type of 60 virtually , in one or more networks , such as Private , Com service ( e.g. , storage , processing , bandwidth , and active user munity , Public , or Hybrid clouds as described hereinabove , accounts ) . Resource usage can be monitored , controlled , and or a combination thereof . This allows cloud computing reported , providing transparency for both the provider and environment 50 to offer infrastructure , platforms and / or 
 consumer of the utilized service . software as services for which a cloud consumer does not 
 Service Models are as follows : 65 need to maintain resources on a local computing device . It Software as a Service ( SaaS ) : the capability provided to is understood that the types of computing devices 54A - N the consumer is to use the provider's applications running on shown in FIG . 1 are intended to be illustrative only and that 75 . US 11,295,208 B2 
 7 8 
 computing nodes 10 and cloud computing environment 50 Machine learning functionality can be implemented using an can communicate with any type of computerized device over artificial neural network ( ANN ) having the capability to be any type of network and / or network addressable connection trained to perform a currently unknown function . In machine ( e.g. , using a web browser ) . learning and cognitive science , ANNs are a family of Referring now to FIG . 2 , a set of functional abstraction 5 statistical learning models inspired by the biological neural layers provided by cloud computing environment 50 ( FIG . networks of animals , in particular the brain . ANNs can be 1 ) is shown . It should be understood in advance that the used to estimate or approximate systems and functions that components , layers , and functions shown in FIG . 2 are depend on a large number of inputs . ANNs can be embodied intended to be illustrative only and embodiments of the as so - called “ neuromorphic ” systems of interconnected pro invention are not limited thereto . As depicted , the following 10 cessor elements that act as simulated “ neurons ” and layers and corresponding functions are provided : exchange “ messages ” between each other in the form of Hardware and software layer 60 includes hardware and electronic signals . Similar to the so - called “ plasticity ” of software components . Examples of hardware components synaptic neurotransmitter connections that carry messages include : mainframes 61 ; RISC ( Reduced Instruction Set between biological neurons , the connections in ANNs that Computer ) architecture based servers 62 ; servers 63 ; blade 15 carry electronic messages between simulated neurons are servers 64 ; storage devices 65 ; and networks and networking provided with numeric weights that correspond to the components 66. In some embodiments , software compo- strength or weakness of a given connection . The weights can nents include network application server software 67 and be adjusted and tuned based , at least in part , on experience , database software 68 . making ANNs adaptive to inputs and capable of learning . Virtualization layer 70 provides an abstraction layer from 20 For example , an ANN for handwriting recognition is defined which the following examples of virtual entities may be by a set of input neurons that can be activated by the pixels provided : virtual servers 71 ; virtual storage 72 ; virtual of an input image . After being weighted and transformed by networks 73 , including virtual private networks ; virtual a function determined by the network's designer , the acti applications and operating systems 74 ; and virtual clients vation of these input neurons are then passed to other 25 downstream neurons , which are often referred to as " hidden " In one example , management layer 80 may provide the neurons . This process is repeated until an output neuron is functions described below . Resource provisioning 81 pro- activated . The activated output neuron determines which vides dynamic procurement of computing resources and character was read . other resources that are utilized to perform tasks within the Machine learning is often employed by numerous tech cloud computing environment . Metering and Pricing 82 30 nologies to determine inferences and / or relationships among provide cost tracking as resources are utilized within the digital data . For example , machine learning technologies , cloud computing environment , and billing or invoicing for signal processing technologies , image processing technolo consumption of these resources . In one example , these gies , data analysis technologies , and / or other technologies resources may include application software licenses . Secu- employ machine learning models to analyze digital data , rity provides identity verification for cloud consumers and 35 process digital data , determine inferences from digital data , tasks , as well as protection for data and other resources . User and / or determine relationships among digital data . portal 83 provides access to the cloud computing environ- A deep neural network ( DNN ) is a type of ANN that has ment for consumers and system administrators . Service level multiple hidden layers between the input and output layers . management 84 provides cloud computing resource alloca- DNNs can model complex non - linear relationships . DNN tion and management such that required service levels are 40 architectures generate compositional models where the met . Service Level Agreement ( SLA ) planning and fulfill- object is expressed as a layered composition of primitives . ment 85 provide pre - arrangement for , and procurement of , The extra layers enable composition of features from lower cloud computing resources for which a future requirement is layers , potentially modeling complex data with fewer units anticipated in accordance with an SLA . than a similarly performing shallow network . Some DNNS Workloads layer 90 provides examples of functionality 45 are feedforward networks in which data flows from the input for which the cloud computing environment may be utilized . layer to the output layer without looping back . Recurrent Examples of workloads and functions which may be pro- neural networks ( RNNs ) are a further type of DNN , in which vided from this layer include : mapping and navigation 91 ; data can flow in any direction . RNNs are sometimes used for software development and lifecycle management 92 ; virtual applications such as language modeling . Long short - term classroom education delivery 93 ; data analytics processing 50 memory ( LSTM ) networks are another type of DNN . 94 ; transaction processing 95 ; and gradient compression The use of neural networks , particularly with convolu processing 96 . tional layers , has driven progress in deep learning . Such Turning now to an overview of technologies that are more neural networks are referred to as convolutional neural specifically relevant to aspects of the invention , highly networks ( CNN ) . In a CNN , kernels convolute overlapping distributed training of DNNs are often communication con- 55 regions in a visual field , and accordingly emphasize the strained . To overcome this limitation , new gradient com- importance of spatial locality in feature detection . Comput pression techniques are needed that are computationally ing the convolutional layers of the CNN typically encom friendly , applicable to a wide variety of layers seen in DNN , passes more than 90 % of computation time in neural net and adaptable to variations in network architectures as well work training and inference . The training time , for example , as their hyper - parameters . 60 depends on the size of the training dataset that is being used , As previously noted herein , the phrase " machine learn- may be a week or longer . In order to improve training time ing ” broadly describes a function of electronic systems that over single - node systems , distributed systems have been learn from data . A machine learning system , engine , or developed to distribute training data over multiple central module can include a trainable machine learning algorithm processing units ( CPUs ) or graphics processing units that can be trained , such as in an external cloud environment , 65 ( GPUs ) . Ring - based system topologies have been proposed to learn functional relationships between inputs and outputs , to attempt to maximize inter - accelerator bandwidths by wherein the functional relationships are currently unknown . connecting accelerators and / or learners in the system in a a 
 a a US 11,295,208 B2 
 9 10 
 ring network . The accelerator then transports its computed Turning now to a more detailed description of aspects of weight gradients from local mini - batch directly to the adja- the present invention , FIG . 3 illustrates a high level block cent accelerator or with a centralized parameter server . diagram showing an example of a computer - based system However , as the number of learners that are utilized 300 useful for implementing one or more embodiments of increases , distribution of the mini - batch data under strong 5 the invention . Although one exemplary computer system scaling conditions has the adverse effect of significantly 300 is shown , computer system 300 includes a communi increasing the demand for communication bandwidth cation path 326 , which connects computer system 300 to between the learners while proportionally decreasing the additional systems and may include one or more wide area flops needed in each learner , therefore , creating a severe networks ( WAN ) and / or local area networks ( LAN ) such computation to communication imbalance . Thus , accelerat- 10 as the internet , intranet ( s ) , and / or wireless communication ing the CNN training via compression , as described by the network ( s ) . Computer system 300 and additional system are examples of the technical solutions herein , is a desirable in communication via communication path 326 , ( e.g. , to improvement . communicate data between them ) . Turning now to an overview of the aspects of the inven- Computer system 300 includes one or more processors , tion , one or more embodiments of the invention addresses 15 such as processor 302. Processor 302 is connected to a the above - described shortcomings of the prior art by pro- communication infrastructure 304 ( e.g. , a communications viding a new compression technique that assists in mini- bus , cross - over bar , or network ) . Computer system 300 can mizing the amount of data exchanged among accelerators . In include a display interface 306 that forwards graphics , text , particular , an adaptive residual gradient compression and other data from communication infrastructure 304 ( or scheme is provided that utilizes localized selection of gra- 20 from a frame buffer not shown ) for display on a display unit dient residues , in which the residual gradient compression 308. Computer system 300 also includes a main memory scheme is able to automatically tune the compression rate 310 , preferably random access memory ( RAM ) , and may based on local activity . For example , as there may be a lack also include a secondary memory 312. Secondary memory of correlation between the activity of input features and the 312 may include , for example , a hard disk drive 314 and / or residual gradients in any layer , in some embodiments of the 25 a removable storage drive 316 , representing , for example , a present invention , the compression scheme is configured to floppy disk drive , a magnetic tape drive , or an optical disk capture the residues across an entire layer by dividing the drive . Removable storage drive 316 reads from and / or writes entire residue vector , for each layer , uniformly into several to a removable storage unit 318 in a manner well known to bins , thus creating a new hyper - parameter having a fixed those having ordinary skill in the art . Removable storage length bin size , Lj . In each bin , the compression algorithm 30 unit 318 represents , for example , a floppy disk , a compact first finds the maximum of the absolute value of the residue . disc , a magnetic tape , or an optical disk , etc. which is read In addition to this value , the compression algorithm sends by and written to by removable storage drive 316. As will be other residues that are relatively similar in magnitude to this appreciated , removable storage unit 318 includes a computer maximum . A residue is computed for each mini - batch as the readable medium having stored therein computer software sum of the previous residue and the latest gradient value 35 and / or data . obtained from backpropagation . If the sum of its previous In some alternative embodiments of the invention , sec residue plus its latest gradient , with a scale - factor , exceeds ondary memory 312 may include other similar means for the maximum in the bin , then those additional residues are allowing computer programs or other instructions to be included in the set of values to be sent and / or centrally loaded into the computer system . Such means may include , updated . Various suitable scale factors may be used in 40 for example , a removable storage unit 320 and an interface accordance with one or more embodiments of the present 322. Examples of such means may include a program invention . For example , in some embodiments of the present package and package interface ( such as that found in video invention the scale - factor ranges from about 1.5 to about 3 . game devices ) , a removable memory chip ( such as an In some embodiments of the present invention , the scale- EPROM or PROM ) and associated socket , and other remov factor is 2 . 45 able storage units 320 and interfaces 322 which allow As residues may be empirically larger than gradients , one software and data to be transferred from the removable or more of the above - described aspects of the invention storage unit 320 to computer system 300 . address the shortcomings of the prior art by providing a Computer system 300 may also include a communica compression scheme that allows for the sending of important tions interface 324. Communications interface 324 allows residues that are close to a local maximum . By quantizing a 50 software and data to be transferred between the computer compressed residue vector in accordance to one or more system and external devices . Examples of communications embodiments of the present invention , the overall compres- interface 324 may include a modem , a network interface sion rate can be increased . The compression scheme can be ( such as an Ethernet card ) , a communications port , or a applied to every layer separately at each learner . In some PCM - CIA slot and card , etcetera . Software and data trans embodiments of the present invention , each learner sends a 55 ferred via communications interface 324 are in the form of scale - factor in addition to the compressed sparse vector . In signals which may be , for example , electronic , electromag some embodiments of the present invention , by exploiting netic , optical , or other signals capable of being received by both sparsity and quantization , end - to - end compression rates communications interface 324. These signals are provided to of about 200x for fully - connected and recurrent layers and communications interface 324 via communication path ( i.e. , 40x for convolution layers may be achieved without notice- 60 channel ) 326. Communication path 326 carries signals and able degradation in model accuracy ( e.g. , < 1 % degradation ) . may be implemented using wire or cable , fiber optics , a Prior methods had significant degradation , such as about phone line , a cellular phone link , an RF link , and / or other 1.6 % for large networks . In one or more embodiments of the communications channels . present invention , the compression scheme does not require In the present disclosure , the terms “ computer program sorting , or approximation to sorting , and thus the compres- 65 medium , " " computer usable medium , ” and “ computer read sion scheme is able to be computably efficient ( O ) ( N ) ) for able medium ” are used to generally refer to media such as high - performance systems . main memory 310 and secondary memory 312 , removable a 
 a 
 a 
 a 
 a US 11,295,208 B2 
 11 12 
 storage drive 316 , and a hard disk installed in hard disk drive tan h ( ) . The connections between each layer , and thus an 314. Computer programs ( also called computer control entire network , can be represented as a series of matrices . logic ) are stored in main memory 310 , and / or secondary Training the neural network includes finding proper values memory 312. Computer programs may also be received via for these matrices . communications interface 324. Such computer programs , 5 While fully - connected neural networks are able , when when run , enable the computer system to perform the properly trained , to recognize input patterns , such as hand features of the present disclosure as discussed herein . In writing , they may fail to take advantage of shape and particular , the computer programs , when run , enable pro- proximity when operating on input . For example , because cessor 302 to perform the features of the computer system . every pixel is operated on independently , the neural network Accordingly , such computer programs represent controllers 10 may ignore adjacent pixels . A CNN , in comparison , operates of the computer system . by associating an array of values , rather than a single value , FIG . 4 illustrates a block diagram of an example neural with each neuron . Conceptually , the array is a subset of the network in accordance with one or more embodiments of the input pattern , or other parts of the training data . The trans present disclosure , which is interpreting a sample input map formation of a neuron value for the subsequent layer is 400. This particular example uses a handwritten letter “ w ” as 15 generated using convolution . Thus , in a CNN the connection an input map , however , it is understood that other types of strengths are convolution kernels rather than scalar values as input maps are possible . In the illustrated example , the input in a full - network . map 400 is used to create a set of values for the input layer FIG . 5 illustrates an example convolutional layer 500 in 410 , or “ layer - 1 . " In some embodiments of the present a CNN being trained using training data that includes input invention , layer - 1 is generated by direct mapping of a pixel 20 maps 510 and convolution kernels 520 , in accordance with of the sample input map 400 to a particular neuron in one or more embodiments of the present invention . The layer - 1 , such that the neuron shows a 1 or a 0 depending on input maps 510 include multiple input patterns , for example whether the pixel exhibits a particular attribute . Another N input maps . Each input map is a matrix , such as a square example method of assigning values to neurons is discussed matrix of size nxn . The input maps are convolved with below with reference to convolutional neural networks . 25 convolution kernels 520 of size kxk as illustrated to produce Depending on the vagaries of the neural network and the Moutput maps 530 of size n - k + 1xn - k + 1 . Each convolution problem it is created to solve , each layer of the network may is a 3D convolution involving the N input maps . It should be have differing numbers of neurons , and these may or may noted that the input maps , kernels , and output maps need not not be related to particular qualities of the input data . be square . A CNN can include multiple such layers , where Referring to FIG . 4 , neurons in layer - 1 410 are connected 30 the output maps 530 from a previous layer are used as input to neurons in the next layer , layer - 2 420. In a neural network , maps 510 for a subsequent layer . The backpropagation each of the neurons , in a particular layer , is connected to algorithm can be used to learn the weight values of the neurons in the next layer . In this example , a neuron in layer - 2 kxkxMXN filters . receives an input value from each of the neurons in layer - 1 . For example , in some embodiments of the present inven The input values are then summed and this sum is compared 35 tion , the input maps 510 are convolved with each filter bank to a bias . If the value exceeds the bias for a particular neuron , to generate a corresponding output map . For example , in that neuron then holds a value which can be used as input to case the CNN 500 is being trained to identify handwriting , neurons in the next layer of neurons . This computation the input maps 510 are combined with a filter bank that continues through the various layers 430-450 of the neural includes convolution kernels representing a vertical line . network , until it reaches a final layer 460 , referred to as 40 The resulting output map 530 identifies vertical lines which " output ” in FIG . 4. In an example of a neural network used may be present in the input maps 510. Further , another filter for character recognition , each value in the layer is assigned bank may include convolution kernels representing a diago to a particular character . In some embodiments of the present nal line , such as going up and to the right . An output map invention , the network is configured to end with the output 530 resulting from a convolution of the input maps 510 with layer having only one large positive value in one neuron , 45 the second filter bank identifies samples of the training data which then demonstrates which character the network has that contain diagonal lines . The two output maps 530 show computed to be the most likely handwritten input character . different information for the character , while preserving In some embodiments of the present invention , data pixel adjacency . This can result in more efficient character values for each layer in the neural network are represented recognition as vectors or matrices ( or tensors in some examples ) and 50 FIG . 6 illustrates a block diagram of a system 600 for computations are performed as vector or matrix computa- performing adaptive residual gradient compression for train tions . The indexes ( and / or sizes ) of the matrices vary from ing of a deep learning neural network according to one or layer to layer and network to network , as illustrated in FIG . more embodiments of the present invention . System 600 4. Different implementations orient the matrices , or map includes a plurality of learner processing systems 610 , 620 , matrices , to computer memory differently . In the example 55 630 , 640 which are responsible for preforming deep network neural network illustrated in FIG . 4 , each level is a matrix of learning , for example , as an instance of a non - convex neuron values by matrix dimensions for each layer of the optimization problem . This may be useful to train deep neural network . The values in a matrix at a layer are neural nets with a large number of parameters on large multiplied by connection strengths , which are in a transfor- datasets . For example , in some embodiments of the present mation matrix . This matrix multiplication scales each value 60 invention , when multi - workers ( e.g. , plurality of learner in the previous layer according to the connection strengths , processing systems 610 , 620 , 630 , 640 ) train one neural and then is summed . A bias matrix is then added to the network , each worker computes a subset of training data and resulting product matrix to account for the threshold of each communication among workers may be required ; this is neuron in the next level . An activation function is then usually called data - parallelism . To save communication applied to each resultant value , and the resulting values are 65 bandwidth , each worker sends partial value of gradients and placed in the matrix for the next layer . In an example , the keep the reminding residues locally . System 600 provides a activation function may be rectified linear units , sigmoid , or compression technique that assists in minimizing the the 
 a a 
 a US 11,295,208 B2 
 13 14 
 amount of data exchanged among accelerators . In particular , the output maps 530 , as to how close or far off of the CNN system 600 employs an adaptive residual gradient compres- was to the expected . At block 712 , the degree of error with sion scheme that utilizes localized selection of gradient relation to each of the matrices and / or vectors that make up residues , in which the compression scheme is configured to the CNN is determined using gradient descent . Determining capture the residues across an entire layer by dividing the 5 the relative errors is referred to as a “ backward pass ” ( e.g. , entire residue vector for each layer uniformly into several by learner processing systems 610 , 620 , 630 , 640 ) . bins , wherein the fixed length bin size , Ly , is a new hyper- At block 714 , a current gradient vector is obtained on a parameter . In each bin , the maximum of the absolute value layer - by - layer basis by each learner of the system ( e.g. , by of the residue is identified and exchanged among the plu- learner processing systems 610 , 620 , 630 , 640 ) . In some rality of learner processing systems 610 , 620 , 630 , 640 10 embodiments of the present invention , the current gradient and / or transmitted to a parameter server . Other residues that vector for each given neural network layer includes gradient are relatively similar in magnitude to this maximum are also weights of parameters of the given neural network layer . As exchanged among the plurality of learner processing sys- will be discussed in further detail below , in some embodi tems 610 , 620 , 630 , 640 and / or transmitted to a parameter ments of the present invention , the gradient weights are server . For example , in some embodiments of the present 15 calculated from a mini - batch of training data as opposed to invention , a residue that is computed for each mini - batch by from the entire training data . summing a previous residue and a latest gradient value At block 716 , a current residue vector and a compressed obtained from backpropagation . If the sum of its previous current residue vector are generated for each given layer by residue plus its latest gradient , with a scale - factor , exceeds each learner of the system ( e.g. , by learner processing the maximum in the bin , those additional residues are 20 systems 610 , 620 , 630 , 640 ) . In some embodiments of the included in the set of values to be sent and / or centrally present invention , the compressed current residue vector is updated at a server , such as a parameter server . It should be a layer - wise or chunk - wise compressed current residue appreciated that , although four learner processing systems vector . In some embodiments of the present invention , the 610 , 620 , 630 , 640 are illustrated in FIG . 6 , the present current residue vector includes residual gradient weights for techniques may be utilized with any suitable number of 25 a given layer of a mini - batch . In some embodiments of the learner processing systems . In some embodiments of the present invention , the current residue vector is generated by present invention , when multi - workers ( e.g. , plurality of summing the current gradient vector with a prior residue learner processing systems 610 , 620 , 630 , 640 ) train one vector ( e.g. , a residue vector of a prior mini - batch ) . In some neural network , each worker computes the subset of training embodiments of the present invention , the prior residue data and communication among workers are required ; this is 30 vector is an empty vector or has null values , which may usually called data - parallelism . To save communication occur when a run is first initialized . As such , in some bandwidth , each worker sends partial value of gradients and embodiments of the present invention , the summation of the keep the reminding residues locally current gradient vector and a prior residue vector results in FIG . 7 illustrates an example process flow 700 for training a current residue vector being obtained that is the same as a DNN , such as a CNN with one or more convolutional 35 the current gradient vector . layers 500 in accordance with one or more embodiments of In some embodiments of the present invention , the com the present invention . The example logic may be imple- pressed current residue vector that is generated at block 716 mented by one or more processors , such as a CPU , a GPU , is generated based , at least in part , on dividing the residual a digital signal processor ( DSP ) , or any other processor or a gradient weights of the current residue vector into a plurality combination thereof . At 702 , the CNN is initialized . In some 40 of bins , of a uniform size , and then quantizing a subset of the embodiments of the present invention , the CNN is initialized residual gradient weights of one or more bins of the plurality with random weights . At 704 , training data for the CNN 500 of bins , in which the uniform size is a hyper - parameter of the is received . In some embodiments of the present invention , neural network . In some embodiments of the present inven the CNN is pre - set with sample convolutional kernels and tion , the quantizing of the subset of the residual gradient biases , which can be refined to provide consistent and 45 weights is based at least in part on calculating a scaling efficient results . In some embodiments of the present inven- parameter for the mini - batch and calculating a local maxi tion , the training data includes a plurality of input training mum of each bin . samples 400 such as for example , on the order of tens of At block 718 , the compressed current residue vectors are thousands of input training samples 400. The input training exchanged among each learner of the system ( e.g. , by samples 400 are associated with an expected output 460. In 50 learner processing systems 610 , 620 , 630 , 640 ) and / or some embodiments of the present invention , the inputs 400 transmitted to a parameter server . In some embodiments of are handwriting samples and the expected outputs 460 are an the present invention , the exchange includes each learner of indication of the correct character for interpreting each the system transmitting the compressed current residue handwriting sample . vector to the other learners of the plurality of learners . At In some embodiments of the present invention , DNN 55 block 720 , the compressed current reduce vectors are training includes training via multiple training epochs . For decompressed at each learner of the plurality of learns ( e.g. , example , in some embodiments of the present invention , by learner processing systems 610 , 620 , 630 , 640 ) . After each epoch includes several mini - batches . Accordingly , in decompression , at block 722 each learner of the plurality of some embodiments of the present invention , as shown at learners locally averages the gradients of the decompressed block 706 the process begins at a mini - batch of a training 60 vectors ( e.g. , by learner processing systems 610 , 620 , 630 , epoch by receiving training data ( e.g. , by learner processing 640 ) . systems 610 , 620 , 630 , 640 ) . Using the input maps 510 and In some embodiments of the present invention , the matri the convolutional kernels 520 , the output maps 530 are ces are then modified to adjust for the error , as shown at generated as described herein , as shown at block 708 ( e.g. , block 724 based on the decompressed vectors . For example , by learner processing systems 610 , 620 , 630 , 640 ) . Gener- 65 in some embodiments of the present invention , the convo ating the output maps 530 is commonly referred to as a lution kernels 520 are modified based on the output error " forward pass . ” At 710 , a determination is made , based on information and then the modified kernels are used to a 
 HEG + dW >> H = Residue + 2 * dW 
 end for 
 Over all bins 
 X 
 else US 11,295,208 B2 
 15 16 
 determine modifications for each neural network matrix , and then updating the current residue vector by substituting which is referred to as an “ update pass . ” In some embodi- the given residual gradient weight of the current residue ments of the present invention , the modified convolutional vector with the quantized value . kernels 520 , after being adjusted , are used for a next The following pseudocode describes two algorithms that mini - batch or epoch of the training , unless the training is 5 can be used to implement process flow 700 : deemed completed , as shown at block 726. In some embodi ments of the present invention , the modified convolutional kernels 520 from one mini - batch are used in a subsequent Algorithm 1 Computation Steps 
 mini - batch . For example , the training may be deemed com learningNoUpdate ( ) Forward / Backward only pleted if the CNN identifies the inputs according to the 10 serializeGrad ( ) > Collect grad of each layer as a vector expected outputs with a predetermined error threshold . If the pack ( ) AdaComp Compression for each layer training is not yet completed , another training epoch , is exchange ( ) >> Learner receives packed grads from others unpack ( ) AdaComp Decompression for each layer performed using the modified convolutional kernels . averageGradients ( ) > Average among all learners In some embodiments of the present invention , each update Weights ( ) Performed locally at each learner iteration of the “ forward and backward pass ” uses the entire 15 training data . Alternatively , the training data may be divided into mini - batches , or subsets . In a batched training process , the CNN is propagated on a forward pass to determine an output for a mini - batch , as shown at block 708. The error Algorithm 2 Details of packs ( ) 
 function is used to compute how far off the CNN was from 20 G – residue + dw DdW is from serializeGrad ( ) the expected output for the batch , as shown at block 710. A Divide G into bins of size T gradient function is determined for the error function . The for i 1 , length ( G ) / T do Over all bins gradient function , in an example , includes partial derivatives Calculate Smax ( i ) ; > Get largest absolute value in each bin for each entry of each neural network matrix with respect to the error . The gradient function represents how much to 25 for ia 1 , length ( G ) / T do adjust each matrix according to the gradient descent method . for j = 1 , T do Over all indices within each bin 
 index = ( i – 1 ) * T + j The processor subsequently modifies the matrices , including if | H ( index ) 12 Smax ( i ) thens Compare to local max the convolutional kernels and the biases , according to the Gq ( index ) Quantize ( G ( index ) ) gradient function , as shown at block 724. As shown at block add Gq ( index ) to a pack vector ( sent in exchange ( ) ) 
 728 , the “ forward / backward passes ” are repeated as long as 30 residue ( index ) = G ( index ) - G ( index ) 
 there are more mini - batches and / or the CNN is not trained . residue ( index ) = G ( index ) > No transmission The mini - batch may include any fraction of the total number end if of input samples needed to complete a training epoch . end for FIG . 8 illustrates example logic of block 716 for gener ating a compressed current residue vector in accordance 35 with one or more embodiments of the present invention . At Algorithm 1 shows an example gradient weight commu block 802 , a scaled current residue vector is generated at nication scheme in accordance with one or more embodi each learner of the system . The scaled current residue vector ments of the present invention . Algorithm 2 shows an includes scaled residual gradient weights for the given example compression scheme in accordance with one or mini - batch . The scaled current residue vector is generated by 40 more embodiments of the present invention ( referred to in multiplying the current gradient vector by a scaling param- shorthand as “ AdaComp " ) . Algorithm 1 is a gradient weight eter and then summing the prior residue vector with the communication scheme that can be used to test the com multiplied gradient vector . In some embodiments of the pression algorithm of Algorithm 2. Algorithm 2 is encapsu present invention , the scale parameter is calculated by lated within the pack ( ) and unpack ( ) functions of Algorithm minimizing quantization error according to L2 normaliza- 45 2. These two functions may be inserted between the back tion . ward pass step 712 and the weight - update step 722 of At block 804 , the residual gradient weights of the current process flow 700 , for example . The pack / unpack functions residue vector are dividing uniformly into a plurality of bins can be implemented independent of the exchange ( ) func of a uniform size , in which the uniform size is a hyper- tion . The exchange ( ) function used may depend on the parameter of the neural network . At block 806 , a local 50 particular topology of the CNN ( e.g. , ring - based vs. param maximum is identified for each given residual gradient eter - server based ) . weight of each given bin . In some embodiments of the Algorithm 2 provides one example of a quantization present invention , the local maximum of a given bin is the function that may be utilized within the compression maximum absolute value of the residual gradient weights of scheme . A sign bit is used with a scale value to represent an the given bin . 55 original number . In this example , a single scale value is used At block 808 , it is determined for each given residual for each given layer in which the scale value is the absolute gradient weight of each given bin whether the given residual value average of all elements in a game vector for the given gradient weight has a corresponding scaled residual gradient layer . Other suitable quantization functions and / or scale weight that exceeds the local maximum of the given bin . At values may be utilized in one or more embodiments of the block 810 , upon determining that a residual gradient weight 60 present invention . that has a corresponding scaled residual gradient weight In some embodiments of the present invention , Algorithm exceeds the local maximum of the given bin , a quantizing 2 selects up to 10 and 100 elements respectively within each value for the give residual gradient weight is generated and bin through sparsity for bin sizes ( LT ) between 50 and 500 the current residue vector is updated . In some embodiments elements . In some embodiments of the present invention , a of the present invention , the current residue vector is 65 sparse - index representation of 8 - bits is used for L sizes that updated by substituting the given residual gradient weight of are less than 40 elements . In some embodiments of the the current residue vector with the quantized value . At block present invention , a 16 - bit representation is used for large LT end for 
 9 
 T US 11,295,208 B2 
 17 18 
 sizes ( e.g. , greater than 500 elements and / or up to 10K programming languages , including an object oriented pro elements ) . In some embodiments of the present invention , gramming language such as Smalltalk , C ++ , or the like , and 2 - bits of an 8 - bit or 16 - bit representation are used to procedural programming languages , such as the “ C ” pro represent ternarized data values . gramming language or similar programming languages . The In comparison to traditional 32 - bit floating point repre- 5 computer readable program instructions may execute sentations , Algorithm 2 is able to achieve an effective entirely on the user's computer , partly on the user's com compression rate of around 40x for convolution layers and puter , as a stand - alone software package , partly on the user's around 200x for fully contented and recurrent layers . One computer and partly on a remote computer or entirely on the factor that makes Algorithm 2 a robust compress technique remote computer or server . In the latter scenario , the remote is that Algorithm 2 utilizes a self - adjustable threshold . 10 computer may be connected to the user's computer through Algorithm 2 applies a compression scheme that sends addi- any type of network , including a local area network ( LAN ) tional residual gradients that are close to the local maximum or a wide area network ( WAN ) , or the connection may be in each bin , and can therefore automatically adapt based on made to an external computer ( for example , through the the number of important gradients in a mini - batch . Internet using an Internet Service Provider ) . In some The present invention may be a system , a method , and / or 15 embodiments , electronic circuitry including , for example , a computer program product at any possible technical detail programmable logic circuitry , field - programmable gate level of integration . The computer program product may arrays ( FPGA ) , or programmable logic arrays ( PLA ) may include a computer readable storage medium ( or media ) execute the computer readable program instruction by uti having computer readable program instructions thereon for lizing state information of the computer readable program causing a processor to carry out aspects of the present 20 instructions to personalize the electronic circuitry , in order to invention . perform aspects of the present invention . The computer readable storage medium can be a tangible Aspects of the present invention are described herein with device that can retain and store instructions for use by an reference to flowchart illustrations and / or block diagrams of instruction execution device . The computer readable storage methods , apparatus ( systems ) , and computer program prod medium may be , for example , but is not limited to , an 25 ucts according to embodiments of the invention . It will be electronic storage device , a magnetic storage device , an understood that each block of the flowchart illustrations optical storage device , an electromagnetic storage device , a and / or block diagrams , and combinations of blocks in the semiconductor storage device , or any suitable combination flowchart illustrations and / or block diagrams , can be imple of the foregoing . A non - exhaustive list of more specific mented by computer readable program instructions . examples of the computer readable storage medium includes 30 These computer readable program instructions may be the following : a portable computer diskette , a hard disk , a provided to a processor of a general purpose computer , random access memory ( RAM ) , a read - only memory special purpose computer , or other programmable data pro ( ROM ) , an erasable programmable read - only memory cessing apparatus to produce a nine , such that the ( EPROM or Flash memory ) , a static random access memory instructions , which execute via the processor of the com ( SRAM ) , a portable compact disc read - only memory ( CD- 35 puter or other programmable data processing apparatus , ROM ) , a digital versatile disk ( DVD ) , a memory stick , a create means for implementing the functions / acts specified floppy disk , a mechanically encoded device such as punch- in the flowchart and / or block diagram block or blocks . These cards or raised structures in a groove having instructions computer readable program instructions may also be stored recorded thereon , and any suitable combination of the fore- in a computer readable storage medium that can direct a going . A computer readable storage medium , as used herein , 40 computer , a programmable data processing apparatus , and / is not to be construed as being transitory signals per se , such or other devices to function in a particular manner , such that as radio waves or other freely propagating electromagnetic the computer readable storage medium having instructions waves , electromagnetic waves propagating through a wave- stored therein comprises an article of manufacture including guide or other transmission media ( e.g. , light pulses passing instructions which implement aspects of the function / act through a fiber optic cable ) , or electrical signals transmitted 45 specified in the flowchart and / or block diagram block or through a wire . blocks . Computer readable program instructions described herein The computer readable program instructions may also be can be downloaded to respective computing / processing loaded onto a computer , other programmable data process devices from a computer readable storage medium or to an ing apparatus , or other device to cause a series of operational external computer or external storage device via a network , 50 steps to be performed on the computer , other programmable for example , the Internet , a local area network , a wide area apparatus or other device to produce a computer imple network and / or a wireless network . The network may com- mented process , such that the instructions which execute on prise copper transmission cables , optical transmission fibers , the computer , other programmable apparatus , or other wireless transmission , routers , firewalls , switches , gateway device implement the functions / acts specified in the flow computers and / or edge servers . A network adapter card or 55 chart and / or block diagram block or blocks . network interface in each computing / processing device The flowchart and block diagrams in the Figures illustrate receives computer readable program instructions from the the architecture , functionality , and operation of possible network and forwards the computer readable program implementations of systems , methods , and computer pro instructions for storage in a computer readable storage gram products according to various embodiments of the medium within the respective computing / processing device . 60 present invention . In this regard , each block in the flowchart Computer readable program instructions for carrying out or block diagrams may represent a module , segment , or operations of the present invention may be assembler portion of instructions , which comprises one or more instructions , instruction - set - architecture ( ISA ) instructions , executable instructions for implementing the specified logi machine instructions , machine dependent instructions , cal function ( s ) . In some alternative implementations , the microcode , firmware instructions , state - setting data , con- 65 functions noted in the blocks may occur out of the order figuration data for integrated circuitry , or either source code noted in the Figures . For example , two blocks shown in or object code written in any combination of one or more succession may , in fact , be executed substantially concur US 11,295,208 B2 
 19 20 
 rently , or the blocks may sometimes be executed in the generating , by the processor , a scaled current residue reverse order , depending upon the functionality involved . It vector comprising scaled residual gradient weights for will also be noted that each block of the block diagrams the mini batch , wherein generating the scaled current and / or flowchart illustration , and combinations of blocks in residue vector comprises multiplying the current gra the block diagrams and / or flowchart illustration , can be 5 dient vector by the scaling parameter and summing the implemented by special purpose hardware - based systems prior residue vector with the multiplied gradient vector ; that perform the specified functions or acts or carry out dividing the residual gradient weights of the current combinations of special purpose hardware and computer residue vector into the plurality of bins of the uniform instructions . size ; The descriptions of the various embodiments of the 10 identifying , for each bin of the plurality of bins , a local present invention have been presented for purposes of maximum of the absolute value of the residual gradient illustration , but are not intended to be exhaustive or limited weights of the bin ; to the embodiments disclosed . Many modifications and determining , for each residual gradient weight of each variations will be apparent to those of ordinary skill in the bin , that a corresponding scaled residual gradient art without departing from the scope and spirit of the 15 weight of the scaled residue vector exceeds the local described embodiments . The terminology used herein was maximum of the bin ; and chosen to best explain the principles of the embodiments , the upon identifying , for each residual gradient weight of practical application or technical improvement over tech each bin , that the corresponding scaled residual gradi nologies found in the marketplace , or to enable others of ent weight of the scaled residue vector exceeds the ordinary skill in the art to understand the embodiments 20 local maximum of the bin , generating a quantizing described herein . value for the give residual gradient weight and updating 
 What is claimed is : the current residue vector by substituting the residual 1. A computer - implemented method for adaptive residual gradient weight of the current residue vector with the gradient compression for training of a deep learning neural a quantized value . network ( DNN ) , the computer implemented method com- 25 3. The computer - implemented method of claim 2 , prising : wherein the scale parameter is calculated by minimizing obtaining , by a processor of a first learner of a plurality of quantization error according to L2 normalization . learners , a current gradient vector for a neural network 4. The computer - implemented method of claim 2 , layer of the DNN , wherein the current gradient vector wherein : comprises gradient weights of parameters of the neural 30 the DNN includes one or more convolution network network layer that are calculated by training the neural layers ; and network layer of the DNN using a mini - batch of the size of the plurality of bins is set to 50 for the one or training data , wherein training the neural network layer more convolution layers . of the DNN using the mini - batch of training data 5. The computer - implemented method of claim 2 , comprises : 35 wherein : receiving the training data comprising a plurality of the DNN includes at least one of more fully connected input samples ; layers ; and determining a mini - batch from the training data ; the size of the bins is set to 500 for the one or more fully performing a forward pass and a backward pass connected layers . through the DNN to calculate a current gradient 40 6. A system for adaptive residual gradient compression for 
 vector ; and training of a deep learning neural network ( DNN ) , the updating one or more gradient weights for the DNN system comprising a plurality of learners , wherein at least based on the current gradient vector ; one leaner of the plurality of learners is configured to generating , by the processor , a current residue vector perform a method comprising : comprising residual gradient weights for the mini- 45 obtaining a current gradient vector for a neural network batch , wherein generating the current residue vector layer of the DNN , wherein the current gradient vector comprises summing a prior residue vector and the comprises gradient weights of parameters of the neural current gradient vector ; network layer that are calculated by training the neural generating , by the processor , a compressed current resi- network layer of the DNN using a mini - batch of due vector based at least in part on dividing the residual 50 training data , wherein training the neural network layer gradient weights of the current residue vector into a of the DNN using the mini - batch of training data plurality of bins of a uniform size and quantizing a comprises : subset of the residual gradient weights of one or more receiving the training data comprising a plurality of bins of the plurality of bins , wherein quantizing the input samples ; subset of the residual gradient weights is based at least 55 determining a mini - batch from the training data ; in part on calculating a scaling parameter for the performing a forward pass and a backward pass mini - batch and calculating a local maximum of each through the DNN to calculate a current gradient bin , wherein the uniform size of the bins is a hyper vector ; and parameter of the DNN ; updating one or more gradient weights for the DNN transmitting , by the processor , the compressed current 60 based on the current gradient vector ; residue vector to a second learner of the plurality of generating a current residue vector comprising residual learners ; and gradient weights for the mini - batch , wherein generating updating , at each of the plurality of learners , the gradient the current residue vector comprises summing a prior weights of the parameters of the neural network layer . residue vector and the current gradient vector ; 2. The computer - implemented method of claim 1 , 65 generating a compressed current residue vector based at wherein generating the compressed current residue vector least in part on dividing the residual gradient weights of comprises : the current residue vector into a plurality of bins of a 5 
 10 
 15 
 30 US 11,295,208 B2 
 21 22 
 uniform size and quantizing a subset of the residual receiving the training data comprising a plurality of gradient weights of one or more bins of the plurality of input samples ; 
 bins , wherein quantizing the subset of the residual determining a mini - batch from the training data ; gradient weights is based at least in part on calculating performing a forward pass and a backward pass a scaling parameter for the mini - batch and calculating through the DNN to calculate a current gradient 
 a local maximum of each bin , wherein the uniform size vector ; and of the bins is a hyper - parameter of the DNN ; updating one or more gradient weights for the DNN transmitting the compressed current residue vector to a based on the current gradient vector ; second learner of the plurality of learners ; and generating a current residue vector comprising residual updating , at each of the plurality of learners , the gradient gradient weights for the mini - batch , wherein generating weights of the parameters of the neural network layer . the current residue vector comprises summing a prior 7. The system of claim 6 , wherein generating the com residue vector and the current gradient vector ; pressed current residue vector comprises : generating a compressed current residue vector based , at generating , by the processor , a scaled current residue least in part , on dividing the residual gradient weights vector comprising scaled residual gradient weights for of the current residue vector into a plurality of bins of the mini batch , wherein generating the scaled current a uniform size and quantizing a subset of the residual residue vector comprises multiplying the current gra- gradient weights of one or more bins of the plurality of dient vector by the scaling parameter and summing the bins , wherein quantizing the subset of the residual prior residue vector with the multiplied gradient vector ; 20 gradient weights is based at least in part on calculating dividing the residual gradient weights of the current a scaling parameter for the mini - batch and calculating residue vector into the plurality of bins of the uniform a local maximum of each bin , wherein the uniform size size ; of the bins is a hyper - parameter of the DNN ; and identifying , for each bin of the plurality of bins , a local transmitting the compressed current residue vector to a maximum of the absolute value of the residual gradient 25 second learner of the plurality of learners ; and weights of the bin ; updating , at each of the plurality of learners , the gradient determining , for each residual gradient weight of each weights of the parameters of the neural network layer . bin , that a corresponding scaled residual gradient 12. The computer program product of claim 11 , wherein weight of the scaled residue vector exceeds the local generating the compressed current residue vector comprises : maximum of the bin ; and generating , by the processor , a scaled current residue upon identifying , for each residual gradient weight of vector comprising scaled residual gradient weights for each bin , that the corresponding scaled residual gradi ent weight of the scaled residue vector exceeds the the mini batch , wherein generating the scaled current 
 local maximum of the bin , generating a quantizing residue vector comprises multiplying the current gra 
 value for the give residual gradient weight and updating 35 dient vector by the scaling parameter and summing the 
 the current residue vector by substituting the residual prior residue vector with the multiplied gradient vector ; 
 gradient weight of the current residue vector with the dividing the residual gradient weights of the current 
 quantized value . residue vector into the plurality of bins of the uniform 
 8. The system of claim 7 , wherein the scale parameter is size ; calculated by minimizing quantization error according to L2 40 identifying , for each bin of the plurality of bins , a local 
 normalization . maximum of the absolute value of the residual gradient 9. The system of claim 7 , wherein : weights of the bin ; 
 the DNN includes one or more convolution network determining , for each residual gradient weight of each layers ; and bin , that a corresponding scaled residual gradient the size of the plurality of bins is set to 50 for the one or 45 weight of the scaled residue vector exceeds the local more convolution layers . maximum of the bin ; and 10. The system of claim 7 , wherein : upon determining , for each residual gradient weight of the DNN includes at least one of more fully connected each bin , that the corresponding scaled residual gradi layers ; and ent weight of the scaled residue vector exceeds the the size of the bins is set to 500 for the one or more fully 50 local maximum of the bin , generating a quantizing connected layers . value for the residual gradient weight and updating the 11. A computer program product for adaptive residual current residue vector by substituting the residual gra gradient compression for training of a deep learning neural dient weight of the current residue vector with the network ( DNN ) , the computer program product comprising quantized value . a computer readable storage medium having program 55 13. The computer program product of claim 12 , wherein instructions embodied therewith , the program instructions the scale parameter is calculated by minimizing quantization executable by a processor of at least a first leaner of a error according to L2 normalization . plurality of learners to cause the first learner to perform a 14. The computer program product of claim 12 , wherein : method comprising : the DNN includes one or more convolution network obtaining a current gradient vector for a neural network 60 layers ; and layer of the DNN , wherein the current gradient vector the size of the plurality of bins is set to 50 for the one or comprises gradient weights of parameters of the neural more convolution layers . network layer that are calculated by training the neural 15. The computer program product of claim 12 , wherein : network layer of the DNN using a mini - batch of the DNN includes at least one of more fully connected training data , wherein training the neural network layer 65 layers ; and of the DNN using the mini - batch of training data the size of the bins is set to 500 for the one or more fully comprises : connected layers . US 11,295,208 B2 
 23 24 
 16. A computer - implemented method for training a deep ent weight of the scaled residue vector exceeds the learning neural network ( DNN ) via adaptive residual gradi- local maximum of the bin , generating a quantizing ent compression , the computer implemented method com value for the residual gradient weight and updating the prising : current residue vector by substituting the residual gra receiving , by a system comprising a plurality of learners , 5 dient weight of the current residue vector with the training data for training of the DNN using one or more quantized value . neural network layers ; 18. The computer - implemented method of claim 17 , generating , at each learner of the plurality of learners , a wherein the scale parameter is calculated by minimizing current gradient vector for a neural network layer , wherein the current gradient vector comprises gradient 10 quantization error according to L2 normalization . 
 weights of parameters of the neural network layer , 19. The computer - implemented method of claim 17 , wherein : wherein the gradient weights of parameters of the the DNN includes one or more convolution network neural network layer are calculated by training the neural network layer using a mini - batch of training layers ; and 
 data , wherein training the neural network layer of the 15 the size of the plurality of bins is set to 50 for the one or 
 DNN using the mini - batch of training data comprises : more convolution layers . receiving the training data comprising a plurality of 20. The computer - implemented method of claim 17 , input samples ; wherein : 
 determining a mini - batch from the training data ; the DNN includes at least one of more fully connected performing a forward pass and a backward pass 20 layers ; and through the DNN to calculate a current gradient the size of the bins is set to 500 for the one or more fully 
 vector ; and connected layers . updating one or more gradient weights for the DNN 21. A system for training a deep learning neural network based on the current gradient vector ; ( DNN ) via adaptive residual gradient compression , the sys generating , at each learner of the plurality of learners , a 25 tem comprising a plurality of learners , wherein the system is current residue vector comprising residual gradient configured to perform a method comprising : weights for the mini - batch , wherein generating the receiving training data for training of the DNN using one current residue vector comprises summing a prior resi- or more neural network layers ; due vector and the current gradient vector ; generating , at each learner of the plurality of learners , a generating , at each learner of the plurality of learners , a 30 current gradient vector for a neural network layer , compressed current residue vector based at least in part wherein the current gradient vector comprises gradient on dividing the residual gradient weights of the current weights of parameters of the neural network layer , residue vector into a plurality of bins of a uniform size wherein the gradient weights of parameters of the and quantizing a subset of the residual gradient weights neural network layer are calculated by training the of one or more bins of the plurality of bins , wherein 35 neural network layer using a mini - batch of training quantizing the subset of the residual gradient weights is data , wherein training the neural network layer of the based at least in part on calculating a scaling parameter DNN using the mini - batch of training data comprises : for the mini - batch and calculating a local maximum of receiving the training data comprising a plurality of each bin , wherein the uniform size of the bins is a input samples ; hyper - parameter of the DNN ; and determining a mini - batch from the training data ; exchanging the compressed current residue vectors performing a forward pass and a backward pass among the plurality of learners ; through the DNN to calculate a current gradient decompressing , at each of the plurality of learners , the vector ; and compressed current residue vectors ; and updating one or more gradient weights for the DNN updating , at each of the plurality of learners , the gradient 45 based on the current gradient vector ; weights of the parameters of the neural network layer . generating , at each learner of the plurality of learners , a 17. The computer - implemented method of claim 16 , current residue vector comprising residual gradient wherein generating the compressed current residue vector weights for the mini - batch , wherein computing the comprises : current residue vector comprises summing a prior resi generating a scaled current residue vector comprising 50 due vector and the current gradient vector ; scaled residual gradient weights for the mini batch , generating , at each learner of the plurality of learners , a wherein generating the scaled current residue vector compressed current residue vector based , at least in comprises multiplying the current gradient vector by part , on dividing the residual gradient weights of the the scaling parameter and summing the prior residue current residue vector into a plurality of bins of a vector with the multiplied gradient vector ; uniform size and quantizing a subset of the residual dividing the residual gradient weights of the current gradient weights of one or more bins of the plurality of residue vector into the plurality of bins of the uniform bins , wherein quantizing the subset of the residual size ; gradient weights is based at least in part on calculating identifying , for each bin of the plurality of bins , a local a scaling parameter for the mini - batch and calculating maximum of the absolute value of the residual gradient 60 a local maximum of each bin , wherein the uniform size weights of the bin ; of the bins is a hyper - parameter of the DNN ; and determining , for each residual gradient weight of each exchanging the compressed current residue vectors bin , hat a corresponding scaled residual gradient weight among the plurality of learners ; of the scaled residue vector exceeds the local maximum decompressing , at each of the plurality of learners , the 
 of the bin ; and compressed current residue vectors ; and upon determining , for each residual gradient weight of updating , at each of the plurality of learners , the gradient each bin , that the corresponding scaled residual gradi- weights of the parameters of the neural network layer . 40 
 55 
 65 US 11,295,208 B2 
 25 26 
 22. The system of claim 21 , wherein generating the ent weight of the scaled residue vector exceeds the compressed current residue vector comprises : local maximum of the bin , generating a quantizing generating a scaled current residue vector comprising value for the residual gradient weight and updating the scaled residual gradient weights for the mini batch , current residue vector by substituting the residual gra wherein generating the scaled current residue vector 5 dient weight of the current residue vector with the comprises multiplying the current gradient vector by quantized value . the scaling parameter and summing the prior residue 23. The system of claim 22 , wherein the scale parameter 
 vector with the multiplied gradient vector ; is calculated by minimizing quantization error according to 
 L2 normalization . dividing the residual gradient weights of the current residue vector into the plurality of bins of the uniform 24. The system of claim 22 , wherein : 
 size ; the DNN includes one or more convolution network 
 identifying , for each bin of the plurality of bins , a local layers ; and 
 maximum of the absolute value of the residual gradient the size of the plurality of bins is set to 50 for the one or 
 weights of the bin ; more convolution layers . determining , for each residual gradient weight of each 15 25. The system of claim 22 , wherein : bin , that a corresponding scaled residual gradient the DNN includes at least one of more fully connected 
 weight of the scaled residue vector exceeds the local layers ; and 
 maximum of the bin ; and the size of the bins is set to 500 for the one or more fully connected layers . upon determining , for each residual gradient weight of each bin , that the corresponding scaled residual gradi 10 